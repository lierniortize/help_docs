{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Help_docs For full documentation visit mkdocs.org . Mkdocs mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Deploy to github mkdocs gh-deploy -c","title":"INICIO"},{"location":"#help_docs","text":"For full documentation visit mkdocs.org .","title":"Help_docs"},{"location":"#mkdocs","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Mkdocs"},{"location":"#deploy-to-github","text":"mkdocs gh-deploy -c","title":"Deploy to github"},{"location":"GA/","text":"ALGORITMO GEN\u00c9TICO PSEUDOC\u00d3DIGO INICIACI\u00d3N: Generaci\u00f3n aleatoria de poblaci\u00f3n inicial, estar\u00e1 constituida por un conjunto de cromosomas, los cuales representan las posibles soluciones del problema. Si no se hace aleatoriamente es importante tener diversidad. Sino se puede llegar a una convergencia prematura (un \u00f3ptimo local). EVALUACI\u00d3N: A cada cromosoa se le aplicar\u00e1 una funci\u00f3n de aptitud para saber c\u00f3mo de buena es esta soluci\u00f3n. CONDICI\u00d3N DE T\u00c9RMINO: Dos opciones: Establecer un m\u00e1ximo de iteraciones La poblaci\u00f3n no cambia de iteraci\u00f3n a iteraci\u00f3n Pasos: Selecci\u00f3n. despu\u00e9s de saber la \u00f3ptima aptitud de cada cromosoma, se eligen los que cruzaremos con la siguiente generaci\u00f3n. Cruzamiento. Se recombina. Mutaci\u00f3n. Modificar al azar parte del cromosoma de los individuos para explora nuevas opciones. Reemplazo. Seleccionar los mejores individuos para que conformen la siguiente generaci\u00f3n. EJEMPLO CON TSP (PYTHON) Link al ejemplo Links de inter\u00e9s: Link2 Link3 Link4 Link5 Se quiere buscar la ruta con menos distancia que pase por diferentes ciudades. Condiciones: Hay que pasar por todas las ciudades Hay que acabar en la ciudad donde se ha iniciado la ruta Conceptos GEN: Una ciudad CROMOSOMA (individuo): Una ruta que cuple las condiciones POBLACI\u00d3N: Un conjunto de cromosomas (generaci\u00f3n) PADRES: Dos rutas que se combinan para crear una nueva CONJUNTO DE CRUZADO: Conjunto de padres que ser\u00e1 usado para crear la siguiente poblaci\u00f3n FUNCI\u00d3N DE APTITUD: Funci\u00f3n que nos dice cu\u00e1nto de buena es una ruta (en nuestro caso cu\u00e1nto de corta) MUTACI\u00d3N: Una froma de introducir variaciones en nuestra poblaci\u00f3n cambiando dos ciudades de forma aleatoria ELITISMO: Una forma de llevar los mejores cromosomas a la siguiente generaci\u00f3n Algoritmo 1) Crear poblaci\u00f3n 2) Determinar aptitud 3) Seleccionar conjunto de cruzado 4) Cruzar 5) Mutaci\u00f3n 6) Repetir Python Paquetes import numpy as np, random, operator, pandas as pd, matplotlib.pyplot as plt Dos clases: City y Fitness City : class City: def __init__(self, x, y): self.x = x self.y = y def distance(self, city): xDis = abs(self.x - city.x) yDis = abs(self.y - city.y) distance = np.sqrt((xDis ** 2) + (yDis ** 2)) return distance def __repr__(self): return \"(\" + str(self.x) + \",\" + str(self.y) + \")\" #es solamente notaci\u00f3n para ver m\u00e1s f\u00e1cil las coordenadas de cada ciudad Fitness : class Fitness: def __init__(self, route): self.route = route self.distance = 0 self.fitness= 0.0 def routeDistance(self): if self.distance == 0: pathDistance = 0 for i in range(0, len(self.route)): fromCity = self.route[i] toCity = None if i + 1 < len(self.route): #Poruqe la ruta empieza y acaba en el mismo sitio, esto cuenta el \u00faltimo tramo toCity = self.route[i + 1] else: toCity = self.route[0] pathDistance += fromCity.distance(toCity) self.distance = pathDistance return self.distance def routeFitness(self): if self.fitness == 0: self.fitness = 1 / float(self.routeDistance()) #Cuanto menor es la distancia mejor es la aptitud (mejor fitness) return self.fitness Creamos poblaci\u00f3n Creamos una ruta de forma aleatoria: def createRoute(cityList): route = random.sample(cityList, len(cityList)) return route Como queremos una poblaci\u00f3n creamos muchas rutas: def initialPopulation(popSize, cityList): population = [] for i in range(0, popSize): population.append(createRoute(cityList)) return population ESTO S\u00d3LO SE REALIZAR\u00c1 PARA LA POBLACI\u00d3N INICIAL. A PARTIR DE AQU\u00cd CREAREMOS NUEVAS GENERACIONES MEDIANTE CRUZADO Y MUTACI\u00d3N Determinamos aptitud De aqu\u00ed sacaremos una lista ordenada con el ID de ruta y la aptitud asociada a esta. def rankRoutes(population): fitnessResults = {} for i in range(0,len(population)): fitnessResults[i] = Fitness(population[i]).routeFitness() return sorted(fitnessResults.items(), key = operator.itemgetter(1), reverse = True) Selecionamos el conjunto de cruzado Tenemos dos formas diferentes para seleccionar a los padres que usaremos para crear las siguientes generaciones: Proporcional a la aptitud: La aptitud de cada individuo relativo a la poblaci\u00f3n es usado para determinar la probabilidad de selecci\u00f3n. Podemos llamarlo probabilidad por peso de aptitud (en el ejemplo haremos esto). Selecci\u00f3n por campeonato: Se selecciona aleatoriamente un conjunto de individuos. Y el que mayor aptitud tenga se selecciona como el primero de los padres. Repetimos el proceso para elegir el segundo padre. Tambi\u00e9n usaremos el elitismo. Es decir, los mejores individuos de la poblaci\u00f3n pasar\u00e1n autom\u00e1ticamente a la siguiente generaci\u00f3n. Es decir, crearemos el conjunto de cruzado en dos pasos: Usaremos el output de rankRoute para saber cuales usamos en la siguiente funci\u00f3n def selection(popRanked, eliteSize): selectionResults = [] df = pd.DataFrame(np.array(popRanked), columns=[\"Index\",\"Fitness\"]) #Calculamos el peso relativo de la aptitud df['cum_sum'] = df.Fitness.cumsum() df['cum_perc'] = 100*df.cum_sum/df.Fitness.sum() for i in range(0, eliteSize): #Mantenemos en la selecci\u00f3n las rutas \u00e9lite selectionResults.append(popRanked[i][0]) for i in range(0, len(popRanked) - eliteSize): #Comparamos pesos relativos de aptitud de rutas aleatorias para seleccionar los mejores pick = 100*random.random() for i in range(0, len(popRanked)): if pick <= df.iat[i,3]: selectionResults.append(popRanked[i][0]) break return selectionResults #Una lista con IDs de rutas seleccionadas para crear el conjunto de cruzado Creamos el conjunto de cruzado: def matingPool(population, selectionResults): matingpool = [] for i in range(0, len(selectionResults)): index = selectionResults[i] matingpool.append(population[index]) return matingpool Cruce Como todas las ciudades tienen que aparecer exactamente una vez, utilizaremos una funci\u00f3n que llamaremos ordered crossover . Seleccionaremos una subsecuecia de uno de los padres, y rellenaremos los huecos con el segundo en el mismo orden de aparici\u00f3n pero teniendo en cuenta de que no se pueden repetir. Ejemplo de ordered crossover PADRE1: 1|2|3|4|5|6|7|8|9 PADRE2: 9|8|7|6|5|4|3|2|1 Subsecuencia: \u00b7|\u00b7|\u00b7|\u00b7|\u00b7|6|7|8|\u00b7 Cruzado: 9|5|4|3|2|6|7|8|1 ------- La funci\u00f3n de cruzado: def breed(parent1, parent2): child = [] childP1 = [] childP2 = [] geneA = int(random.random() * len(parent1)) geneB = int(random.random() * len(parent1)) startGene = min(geneA, geneB) endGene = max(geneA, geneB) for i in range(startGene, endGene): #subsecuencia childP1.append(parent1[i]) childP2 = [item for item in parent2 if item not in childP1] #rellenar child = childP1 + childP2 return child Creaci\u00f3n de nueva poblaci\u00f3n def breedPopulation(matingpool, eliteSize): children = [] length = len(matingpool) - eliteSize pool = random.sample(matingpool, len(matingpool)) for i in range(0,eliteSize): #seguimos queriendo mantener las rutas \u00e9lite children.append(matingpool[i]) for i in range(0, length): #usamos la funci\u00f3n de cruzar para rellenar la generaci\u00f3n child = breed(pool[i], pool[len(matingpool)-i-1]) children.append(child) return children Mutaci\u00f3n La mutaci\u00f3n cumple una funci\u00f3n importante en AG, ya que ayuda a evitar la convergencia local mediante la introducci\u00f3n de rutas novedosas que nos permitir\u00e1n explorar otras partes del espacio de soluci\u00f3n. Utilizaremos mutacion por intercambio . Lo haremos de la siguiente forma: en un individuo de poca probabilidad intercambiaremos dos ciudades. def mutate(individual, mutationRate): for swapped in range(len(individual)): if(random.random() < mutationRate): swapWith = int(random.random() * len(individual)) city1 = individual[swapped] city2 = individual[swapWith] individual[swapped] = city2 individual[swapWith] = city1 return individual A\u00f1adimos esas rutas mutadas a la nueva generaci\u00f3n: def mutatePopulation(population, mutationRate): mutatedPop = [] for ind in range(0, len(population)): mutatedInd = mutate(population[ind], mutationRate) mutatedPop.append(mutatedInd) return mutatedPop Repetici\u00f3n Unimos todo en recursividad: def nextGeneration(currentGen, eliteSize, mutationRate): popRanked = rankRoutes(currentGen) selectionResults = selection(popRanked, eliteSize) matingpool = matingPool(currentGen, selectionResults) children = breedPopulation(matingpool, eliteSize) nextGeneration = mutatePopulation(children, mutationRate) return nextGeneration Puesta en marcha def geneticAlgorithm(population, popSize, eliteSize, mutationRate, generations): pop = initialPopulation(popSize, population) print(\"Initial distance: \" + str(1 / rankRoutes(pop)[0][1])) for i in range(0, generations): pop = nextGeneration(pop, eliteSize, mutationRate) print(\"Final distance: \" + str(1 / rankRoutes(pop)[0][1])) bestRouteIndex = rankRoutes(pop)[0][0] bestRoute = pop[bestRouteIndex] return bestRoute Creamos una lista de ciudades y ejecutamos. Hay que ver qu\u00e9 suposiciones funcionan mejor. En este ejemplo, tenemos 100 individuos en cada generaci\u00f3n, mantenemos 20 individuos de \u00e9lite, usamos una tasa de mutaci\u00f3n del 1 % para un gen determinado y recorremos 500 generaciones: cityList = [] for i in range(0,25): cityList.append(City(x=int(random.random() * 200), y=int(random.random() * 200))) geneticAlgorithm(population=cityList, popSize=100, eliteSize=20, mutationRate=0.01, generations=500)","title":"ALGORITMO GENETICO"},{"location":"GA/#algoritmo-genetico","text":"","title":"ALGORITMO GEN\u00c9TICO"},{"location":"GA/#pseudocodigo","text":"INICIACI\u00d3N: Generaci\u00f3n aleatoria de poblaci\u00f3n inicial, estar\u00e1 constituida por un conjunto de cromosomas, los cuales representan las posibles soluciones del problema. Si no se hace aleatoriamente es importante tener diversidad. Sino se puede llegar a una convergencia prematura (un \u00f3ptimo local). EVALUACI\u00d3N: A cada cromosoa se le aplicar\u00e1 una funci\u00f3n de aptitud para saber c\u00f3mo de buena es esta soluci\u00f3n. CONDICI\u00d3N DE T\u00c9RMINO: Dos opciones: Establecer un m\u00e1ximo de iteraciones La poblaci\u00f3n no cambia de iteraci\u00f3n a iteraci\u00f3n Pasos: Selecci\u00f3n. despu\u00e9s de saber la \u00f3ptima aptitud de cada cromosoma, se eligen los que cruzaremos con la siguiente generaci\u00f3n. Cruzamiento. Se recombina. Mutaci\u00f3n. Modificar al azar parte del cromosoma de los individuos para explora nuevas opciones. Reemplazo. Seleccionar los mejores individuos para que conformen la siguiente generaci\u00f3n.","title":"PSEUDOC\u00d3DIGO"},{"location":"GA/#ejemplo-con-tsp-python","text":"Link al ejemplo Links de inter\u00e9s: Link2 Link3 Link4 Link5 Se quiere buscar la ruta con menos distancia que pase por diferentes ciudades. Condiciones: Hay que pasar por todas las ciudades Hay que acabar en la ciudad donde se ha iniciado la ruta","title":"EJEMPLO CON TSP (PYTHON)"},{"location":"GA/#conceptos","text":"GEN: Una ciudad CROMOSOMA (individuo): Una ruta que cuple las condiciones POBLACI\u00d3N: Un conjunto de cromosomas (generaci\u00f3n) PADRES: Dos rutas que se combinan para crear una nueva CONJUNTO DE CRUZADO: Conjunto de padres que ser\u00e1 usado para crear la siguiente poblaci\u00f3n FUNCI\u00d3N DE APTITUD: Funci\u00f3n que nos dice cu\u00e1nto de buena es una ruta (en nuestro caso cu\u00e1nto de corta) MUTACI\u00d3N: Una froma de introducir variaciones en nuestra poblaci\u00f3n cambiando dos ciudades de forma aleatoria ELITISMO: Una forma de llevar los mejores cromosomas a la siguiente generaci\u00f3n","title":"Conceptos"},{"location":"GA/#algoritmo","text":"1) Crear poblaci\u00f3n 2) Determinar aptitud 3) Seleccionar conjunto de cruzado 4) Cruzar 5) Mutaci\u00f3n 6) Repetir","title":"Algoritmo"},{"location":"GA/#python","text":"","title":"Python"},{"location":"GA/#paquetes","text":"import numpy as np, random, operator, pandas as pd, matplotlib.pyplot as plt","title":"Paquetes"},{"location":"GA/#dos-clases-city-y-fitness","text":"City : class City: def __init__(self, x, y): self.x = x self.y = y def distance(self, city): xDis = abs(self.x - city.x) yDis = abs(self.y - city.y) distance = np.sqrt((xDis ** 2) + (yDis ** 2)) return distance def __repr__(self): return \"(\" + str(self.x) + \",\" + str(self.y) + \")\" #es solamente notaci\u00f3n para ver m\u00e1s f\u00e1cil las coordenadas de cada ciudad Fitness : class Fitness: def __init__(self, route): self.route = route self.distance = 0 self.fitness= 0.0 def routeDistance(self): if self.distance == 0: pathDistance = 0 for i in range(0, len(self.route)): fromCity = self.route[i] toCity = None if i + 1 < len(self.route): #Poruqe la ruta empieza y acaba en el mismo sitio, esto cuenta el \u00faltimo tramo toCity = self.route[i + 1] else: toCity = self.route[0] pathDistance += fromCity.distance(toCity) self.distance = pathDistance return self.distance def routeFitness(self): if self.fitness == 0: self.fitness = 1 / float(self.routeDistance()) #Cuanto menor es la distancia mejor es la aptitud (mejor fitness) return self.fitness","title":"Dos clases: City y Fitness"},{"location":"GA/#creamos-poblacion","text":"Creamos una ruta de forma aleatoria: def createRoute(cityList): route = random.sample(cityList, len(cityList)) return route Como queremos una poblaci\u00f3n creamos muchas rutas: def initialPopulation(popSize, cityList): population = [] for i in range(0, popSize): population.append(createRoute(cityList)) return population ESTO S\u00d3LO SE REALIZAR\u00c1 PARA LA POBLACI\u00d3N INICIAL. A PARTIR DE AQU\u00cd CREAREMOS NUEVAS GENERACIONES MEDIANTE CRUZADO Y MUTACI\u00d3N","title":"Creamos poblaci\u00f3n"},{"location":"GA/#determinamos-aptitud","text":"De aqu\u00ed sacaremos una lista ordenada con el ID de ruta y la aptitud asociada a esta. def rankRoutes(population): fitnessResults = {} for i in range(0,len(population)): fitnessResults[i] = Fitness(population[i]).routeFitness() return sorted(fitnessResults.items(), key = operator.itemgetter(1), reverse = True)","title":"Determinamos aptitud"},{"location":"GA/#selecionamos-el-conjunto-de-cruzado","text":"Tenemos dos formas diferentes para seleccionar a los padres que usaremos para crear las siguientes generaciones: Proporcional a la aptitud: La aptitud de cada individuo relativo a la poblaci\u00f3n es usado para determinar la probabilidad de selecci\u00f3n. Podemos llamarlo probabilidad por peso de aptitud (en el ejemplo haremos esto). Selecci\u00f3n por campeonato: Se selecciona aleatoriamente un conjunto de individuos. Y el que mayor aptitud tenga se selecciona como el primero de los padres. Repetimos el proceso para elegir el segundo padre. Tambi\u00e9n usaremos el elitismo. Es decir, los mejores individuos de la poblaci\u00f3n pasar\u00e1n autom\u00e1ticamente a la siguiente generaci\u00f3n. Es decir, crearemos el conjunto de cruzado en dos pasos: Usaremos el output de rankRoute para saber cuales usamos en la siguiente funci\u00f3n def selection(popRanked, eliteSize): selectionResults = [] df = pd.DataFrame(np.array(popRanked), columns=[\"Index\",\"Fitness\"]) #Calculamos el peso relativo de la aptitud df['cum_sum'] = df.Fitness.cumsum() df['cum_perc'] = 100*df.cum_sum/df.Fitness.sum() for i in range(0, eliteSize): #Mantenemos en la selecci\u00f3n las rutas \u00e9lite selectionResults.append(popRanked[i][0]) for i in range(0, len(popRanked) - eliteSize): #Comparamos pesos relativos de aptitud de rutas aleatorias para seleccionar los mejores pick = 100*random.random() for i in range(0, len(popRanked)): if pick <= df.iat[i,3]: selectionResults.append(popRanked[i][0]) break return selectionResults #Una lista con IDs de rutas seleccionadas para crear el conjunto de cruzado Creamos el conjunto de cruzado: def matingPool(population, selectionResults): matingpool = [] for i in range(0, len(selectionResults)): index = selectionResults[i] matingpool.append(population[index]) return matingpool","title":"Selecionamos el conjunto de cruzado"},{"location":"GA/#cruce","text":"Como todas las ciudades tienen que aparecer exactamente una vez, utilizaremos una funci\u00f3n que llamaremos ordered crossover . Seleccionaremos una subsecuecia de uno de los padres, y rellenaremos los huecos con el segundo en el mismo orden de aparici\u00f3n pero teniendo en cuenta de que no se pueden repetir. Ejemplo de ordered crossover PADRE1: 1|2|3|4|5|6|7|8|9 PADRE2: 9|8|7|6|5|4|3|2|1 Subsecuencia: \u00b7|\u00b7|\u00b7|\u00b7|\u00b7|6|7|8|\u00b7 Cruzado: 9|5|4|3|2|6|7|8|1 ------- La funci\u00f3n de cruzado: def breed(parent1, parent2): child = [] childP1 = [] childP2 = [] geneA = int(random.random() * len(parent1)) geneB = int(random.random() * len(parent1)) startGene = min(geneA, geneB) endGene = max(geneA, geneB) for i in range(startGene, endGene): #subsecuencia childP1.append(parent1[i]) childP2 = [item for item in parent2 if item not in childP1] #rellenar child = childP1 + childP2 return child","title":"Cruce"},{"location":"GA/#creacion-de-nueva-poblacion","text":"def breedPopulation(matingpool, eliteSize): children = [] length = len(matingpool) - eliteSize pool = random.sample(matingpool, len(matingpool)) for i in range(0,eliteSize): #seguimos queriendo mantener las rutas \u00e9lite children.append(matingpool[i]) for i in range(0, length): #usamos la funci\u00f3n de cruzar para rellenar la generaci\u00f3n child = breed(pool[i], pool[len(matingpool)-i-1]) children.append(child) return children","title":"Creaci\u00f3n de nueva poblaci\u00f3n"},{"location":"GA/#mutacion","text":"La mutaci\u00f3n cumple una funci\u00f3n importante en AG, ya que ayuda a evitar la convergencia local mediante la introducci\u00f3n de rutas novedosas que nos permitir\u00e1n explorar otras partes del espacio de soluci\u00f3n. Utilizaremos mutacion por intercambio . Lo haremos de la siguiente forma: en un individuo de poca probabilidad intercambiaremos dos ciudades. def mutate(individual, mutationRate): for swapped in range(len(individual)): if(random.random() < mutationRate): swapWith = int(random.random() * len(individual)) city1 = individual[swapped] city2 = individual[swapWith] individual[swapped] = city2 individual[swapWith] = city1 return individual A\u00f1adimos esas rutas mutadas a la nueva generaci\u00f3n: def mutatePopulation(population, mutationRate): mutatedPop = [] for ind in range(0, len(population)): mutatedInd = mutate(population[ind], mutationRate) mutatedPop.append(mutatedInd) return mutatedPop","title":"Mutaci\u00f3n"},{"location":"GA/#repeticion","text":"Unimos todo en recursividad: def nextGeneration(currentGen, eliteSize, mutationRate): popRanked = rankRoutes(currentGen) selectionResults = selection(popRanked, eliteSize) matingpool = matingPool(currentGen, selectionResults) children = breedPopulation(matingpool, eliteSize) nextGeneration = mutatePopulation(children, mutationRate) return nextGeneration","title":"Repetici\u00f3n"},{"location":"GA/#puesta-en-marcha","text":"def geneticAlgorithm(population, popSize, eliteSize, mutationRate, generations): pop = initialPopulation(popSize, population) print(\"Initial distance: \" + str(1 / rankRoutes(pop)[0][1])) for i in range(0, generations): pop = nextGeneration(pop, eliteSize, mutationRate) print(\"Final distance: \" + str(1 / rankRoutes(pop)[0][1])) bestRouteIndex = rankRoutes(pop)[0][0] bestRoute = pop[bestRouteIndex] return bestRoute Creamos una lista de ciudades y ejecutamos. Hay que ver qu\u00e9 suposiciones funcionan mejor. En este ejemplo, tenemos 100 individuos en cada generaci\u00f3n, mantenemos 20 individuos de \u00e9lite, usamos una tasa de mutaci\u00f3n del 1 % para un gen determinado y recorremos 500 generaciones: cityList = [] for i in range(0,25): cityList.append(City(x=int(random.random() * 200), y=int(random.random() * 200))) geneticAlgorithm(population=cityList, popSize=100, eliteSize=20, mutationRate=0.01, generations=500)","title":"Puesta en marcha"},{"location":"airflow/","text":"APACHE AIRFLOW Es una herramienta para crear, planificar y monitorizar fluos de trabajo (DAGs) Las secuencias de tareas pueden ser ejecutadas por planificaci\u00f3n (el desarrollador especificado cu\u00e1ndo se ejecutara, puntual o periodicamente) o evento (ha encontrado nuevos archivo o nuevos datos, ha acabado otro proceso...) Un DAG (Gr\u00e1fico Aciclico Dirigido) es un esquema que nos dice que se tiene que ejecutar cada vez. Apache airflow nos deja programar en Python, nos deja ejecutar, panificar y distribuir tareas. Adem\u00e1s podemos monitorear, hacer loggings y lanzar alertas. Se pueden hacer pruebas unitarias. Se pueden crear plugins seg\u00fan tus necesidades. No tiene soporte nativo para Windows. INICIAR Haremos el host de airflow en Google Cloud composer. Instalar airflow (cuidado con las versiones!): pip install \"apache-airflow==2.2.3\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.2.3/constraints-3.9.txt\" APLICACIONES Dep\u00f3sito de datos: limpieza, organizaci\u00f3n, verificaci\u00f3n de calidad de datos... Machine Learning: Flujos de dato automaticos Growth analytics Search Data infrastructure maintenance OPERATORS Para llevar a cabo cada actividad tendremos un operador diferente. Si no existe la actividad que queramos siempre podremos crear nuestra propia funci\u00f3n. Los operators son los encargados de llevar a cabo estas actividades. A continuaci\u00f3n se explica para qu\u00e9 son algunos de ellos. Link to all the operators GoogleCloudStorageObjectSensor Sensor para ver si un fichero existe. Documentaci\u00f3n GCSToBigQueryOperator Para pasar a BigQuery. Documentaci\u00f3n * ```write_disposition``` [Doc](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.WriteDisposition.html) * ```time_partitioning``` [Doc](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.table.TimePartitioning.html) BigQueryInsertJobOperator Doc PythonVirtualenvOperator Para enviar emails. Doc BashOperator Enviar comando PythonOperator Para escribir c\u00f3digo python y usar sus m\u00f3dulos (p.e. NumPy, Pandas) EmailOperator Enviar emails SimpleHttpOperator Post, Get... MySqlOperator DockerOperator Sensores Si tengo un DAG que no se tiene que ejecutar por tiempo sino por evento, el sensor estar\u00e1 escuchando, y entonces es cuando se ejecutar\u00e1 el DAG. EJEMPLO Se programar\u00e1 el DAG que lleve a cabo lo siguiente: Escuchar a que nos llegue el archivo (evento) > query > cargar un archivo propio > computar (aplicar modelos...) > subir data > enviar correo de confirmaci\u00f3n Descargar data > Enviar esos datos a otro sitio para ser procesados > Monitorear cuando el proceso se complete o durante > Obtener resultado y generar un informe > Enviar el informe por email EJEMPLO EN PSEUDOC\u00d3DIGO imports options start_date default_args with DAG(...) as dag: tasks En qu\u00e9 orden queremos que se ejecuten los tasks. Para ejecutar a la vez [task1, task2] Para ejecutar una detr\u00e1s de otra: task1 >> task2 Options for schedule interval: None, @once, @hourly, @daily, @weekly, @monthly, @yearly EJEMPLO EN GITHUB/VERLAR El repositorio airflow_code est\u00e1 organizado de la siguiente manera: | apache_beam_code/ | credentials/ | python_functions/ | queries/ | schemas/ | send_error_mail/ | __init__.py | dag1.py | dag2.py | ... Los archivos con los DAG est\u00e1n en el root m\u00e1s alto, despu\u00e9s tenemos las carpetas con los c\u00f3digos que utilizar\u00e1n dichos DAGs. Ejemplo de un DAG con los eventos explicados arriba: github/VERLAR","title":"AIRFLOW"},{"location":"airflow/#apache-airflow","text":"Es una herramienta para crear, planificar y monitorizar fluos de trabajo (DAGs) Las secuencias de tareas pueden ser ejecutadas por planificaci\u00f3n (el desarrollador especificado cu\u00e1ndo se ejecutara, puntual o periodicamente) o evento (ha encontrado nuevos archivo o nuevos datos, ha acabado otro proceso...) Un DAG (Gr\u00e1fico Aciclico Dirigido) es un esquema que nos dice que se tiene que ejecutar cada vez. Apache airflow nos deja programar en Python, nos deja ejecutar, panificar y distribuir tareas. Adem\u00e1s podemos monitorear, hacer loggings y lanzar alertas. Se pueden hacer pruebas unitarias. Se pueden crear plugins seg\u00fan tus necesidades. No tiene soporte nativo para Windows.","title":"APACHE AIRFLOW"},{"location":"airflow/#iniciar","text":"Haremos el host de airflow en Google Cloud composer. Instalar airflow (cuidado con las versiones!): pip install \"apache-airflow==2.2.3\" --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.2.3/constraints-3.9.txt\"","title":"INICIAR"},{"location":"airflow/#aplicaciones","text":"Dep\u00f3sito de datos: limpieza, organizaci\u00f3n, verificaci\u00f3n de calidad de datos... Machine Learning: Flujos de dato automaticos Growth analytics Search Data infrastructure maintenance","title":"APLICACIONES"},{"location":"airflow/#operators","text":"Para llevar a cabo cada actividad tendremos un operador diferente. Si no existe la actividad que queramos siempre podremos crear nuestra propia funci\u00f3n. Los operators son los encargados de llevar a cabo estas actividades. A continuaci\u00f3n se explica para qu\u00e9 son algunos de ellos. Link to all the operators GoogleCloudStorageObjectSensor Sensor para ver si un fichero existe. Documentaci\u00f3n GCSToBigQueryOperator Para pasar a BigQuery. Documentaci\u00f3n * ```write_disposition``` [Doc](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.WriteDisposition.html) * ```time_partitioning``` [Doc](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.table.TimePartitioning.html) BigQueryInsertJobOperator Doc PythonVirtualenvOperator Para enviar emails. Doc BashOperator Enviar comando PythonOperator Para escribir c\u00f3digo python y usar sus m\u00f3dulos (p.e. NumPy, Pandas) EmailOperator Enviar emails SimpleHttpOperator Post, Get... MySqlOperator DockerOperator Sensores Si tengo un DAG que no se tiene que ejecutar por tiempo sino por evento, el sensor estar\u00e1 escuchando, y entonces es cuando se ejecutar\u00e1 el DAG.","title":"OPERATORS"},{"location":"airflow/#ejemplo","text":"Se programar\u00e1 el DAG que lleve a cabo lo siguiente: Escuchar a que nos llegue el archivo (evento) > query > cargar un archivo propio > computar (aplicar modelos...) > subir data > enviar correo de confirmaci\u00f3n Descargar data > Enviar esos datos a otro sitio para ser procesados > Monitorear cuando el proceso se complete o durante > Obtener resultado y generar un informe > Enviar el informe por email","title":"EJEMPLO"},{"location":"airflow/#ejemplo-en-pseudocodigo","text":"imports options start_date default_args with DAG(...) as dag: tasks En qu\u00e9 orden queremos que se ejecuten los tasks. Para ejecutar a la vez [task1, task2] Para ejecutar una detr\u00e1s de otra: task1 >> task2 Options for schedule interval: None, @once, @hourly, @daily, @weekly, @monthly, @yearly","title":"EJEMPLO EN PSEUDOC\u00d3DIGO"},{"location":"airflow/#ejemplo-en-githubverlar","text":"El repositorio airflow_code est\u00e1 organizado de la siguiente manera: | apache_beam_code/ | credentials/ | python_functions/ | queries/ | schemas/ | send_error_mail/ | __init__.py | dag1.py | dag2.py | ... Los archivos con los DAG est\u00e1n en el root m\u00e1s alto, despu\u00e9s tenemos las carpetas con los c\u00f3digos que utilizar\u00e1n dichos DAGs. Ejemplo de un DAG con los eventos explicados arriba: github/VERLAR","title":"EJEMPLO EN GITHUB/VERLAR"},{"location":"beam/","text":"APACHE BEAM Maneja y transforma datos en masivo. Utiliza m\u00e1quinas externas para ello. Soporta batch (los datos tienen un fin) y streaming (los datos est\u00e1n entrando continuamente). Tutorial b\u00e1sico INICIAR Instalar SDK de Python Beam https://beam.apache.org/get-started/quickstart-py/ pip install apache-beam[gcp] Escribir c\u00f3digo en un archivo python importando apahe_beam Se puede ejecutar en local si es peque\u00f1o, sino hay que usar alguna herramienta (nosotros usaremos dataflow) Conceptos b\u00e1sicos Pipeline Un Pipeline es el proceso que queremos seguir. Esto inclue leer datos, transformarlos y escribir un output. Todos los programas de Beam tienen que crear un Pipeline. Adem\u00e1s, habr\u00e1 que especificar las opciones con las que queremos ejecutar el c\u00f3digo y d\u00f3nde queremos hacerlo (--runner). --runner DirectRuner #ejecutarlo en local --runner DataflowRunner #ejecutarlo en cloud PCollection Representa un conjunto de datos distribuido (no tiene orden). Puede ser acotada y que est\u00e9 fija en un archivo o infinita dado que entran datos continuamente. Your pipeline typically creates an initial PCollection by reading data from an external data source, but you can also create a PCollection from in-memory data within your driver program. From there, PCollections are the inputs and outputs for each step in your pipeline. PTransform A PTransform represents a data processing operation, or a step, in your pipeline. Every PTransform takes one or more PCollection objects as input, performs a processing function that you provide on the elements of that PCollection, and produces zero or more output PCollection objects. Proceso Con el c\u00f3digo lo q hacemos es escribir el pipeline que luego se ejecutar\u00e1 en una m\u00e1quina (en nuestro caso Cloud Dataflow). Tembi\u00e9n podr\u00edamos ejecutarlo en local si la cantidad de datos es peque\u00f1a. Ejemplo 1 Tomar\u00e1 datos de una fuente (tambi\u00e9n podr\u00edan ser m\u00e1s), realizaremos transformaciones sobre esos datos, y los escribiremos. ETL (Extract, Transform, Load). Tomaremos el Quijote lo leeremos, contaremos las palabras y mostraremos las 5 m\u00e1s repetidas. main.py: from typing import Tuple import apache_beam as beam import argparse from apache_beam import PCollection from apache_beam.options.pipeline_options import PipelineOptions def main(): parser = argparse.ArgumentParser(description=\"Nuestro primer pipeline\") parser.add_argument(\"--entrada\", help=\"Fichero de entrada\") parser.add_argument(\"--salida\", help=\"Fichero de salida\") def run_pipeline(custom_args, beam_args): entrada = custom_args.entrada salida = custom_args.salida opts = PipelineOptions(beam_args) with beam.Pipeline(options=opts) as p: lineas: PCollection[str] = p | \"Leemos entrada\" >> beam.io.ReadFromText(entrada) # \"En un lugar de La Mancha\" --> [\"En\", \"un\", ...], [...], [...] --> \"En\", \"un\", \"lugar\", .... palabras = lineas | \"Pasamos a palabras\" >> beam.FlatMap(lambda l: l.split()) contadas: PCollection[Tuple[str, int]] = limpiadas | \"Contamos\" >> beam.combiners.Count.PerElement() #\"En\" -> (\"En\", 17) # \"un\" -> (\"un\", 28) palabras_top_lista = contadas | \"Ranking\" >> beam.combiners.Top.Of(n_palabras, key=lambda kv: kv[1]) palabras_top = palabras_top_lista | \"Desenvuelve lista\" >> beam.FlatMap(lambda x: x) formateado: PCollection[str] = palabras_top | \"Formateamos\" >> beam.Map(lambda kv: \"%s,%d\" % (kv[0], kv[1])) formateado | \"Escribimos salida\" >> beam.io.WriteToText(salida) if __name__ == '__main__': main() Ejecutar en local: py main.py --entrada quijote.txt --salida salida.txt --runner DirectRunner Ejemplo 2 Ejemplo b\u00e1sico de un pipeline: import apache_beam as beam from apache_beam import Map from apache_beam.io.textio import ReadFromText, WriteToText from apache_beam.coders.coders import Coder import argparse class LatinCoder(Coder): \"\"\"A coder used for reading and writing strings as Latin-1.\"\"\" def encode(self, value): return value.encode('latin-1') def decode(self, value): return value.decode('latin-1') def is_deterministic(self): return True def csv_to_dict(line): # TODO: realizar modificaciones a cada l\u00ednea return line if __name__ == '__main__': parser = argparse.ArgumentParser() # Argumentos necesarios para ejecutar el Pipeline. parser.add_argument('--runner', required=True) parser.add_argument('--project', required=True) parser.add_argument('--region', required=True) parser.add_argument('--staging_location', required=True) parser.add_argument('--temp_location', required=True) parser.add_argument('--network', required=True) parser.add_argument('--subnetwork', required=True) parser.add_argument('--job_name', required=True) parser.add_argument('--input', required=True) parser.add_argument('--output', required=True) known_args, pipeline_args = parser.parse_known_args() p = beam.Pipeline(runner=\"DataflowRunner\", argv=[\"--project\", known_args.project, \"--staging_location\", known_args.staging_location, \"--temp_location\", known_args.temp_location, \"--save_main_session\", \"True\", \"--region\", known_args.region, \"--network\", known_args.network, \"--subnetwork\", known_args.subnetwork, \"--job_name\", known_args.job_name ]) (p | 'Read' >> ReadFromText(file_pattern=known_args.input, skip_header_lines=1, coder=LatinCoder()) | 'Make Dic' >> Map(csv_to_dict) | 'Write' >> # TODO: a\u00f1adir c\u00f3digo para escribir el resultado utilizando 'WriteToText' ) p.run().wait_until_finish() Ejecutar en cloud Par\u00e1metros necesarios: Explicaci\u00f3n de los par\u00e1metros parser = argparse.ArgumentParser() # Argumentos necesarios para ejecutar el Pipeline. parser.add_argument('--runner', required=True) parser.add_argument('--project', required=True) parser.add_argument('--region', required=True) parser.add_argument('--staging_location', required=True) parser.add_argument('--temp_location', required=True) parser.add_argument('--network', required=True) parser.add_argument('--subnetwork', required=True) parser.add_argument('--job_name', required=True) parser.add_argument('--input', required=True) parser.add_argument('--output', required=True) known_args, pipeline_args = parser.parse_known_args() p = beam.Pipeline(runner=\"DataflowRunner\", argv=[\"--project\", known_args.project, \"--staging_location\", known_args.staging_location, \"--temp_location\", known_args.temp_location, \"--save_main_session\", \"True\", \"--region\", known_args.region, \"--network\", known_args.network, \"--subnetwork\", known_args.subnetwork, \"--job_name\", known_args.job_name ]) Algunos par\u00e1metros por defecto para probar: STAGING_NAME = 'gs://apro-verlar/beam-test01/staging' TEMP_NAME = 'gs://apro-verlar/beam-test01/temp' REGION = 'us-central1' NETWORK = 'default' SUBNETWORK = 'regions/us-central1/subnetworks/default' RUNNER = 'DataflowRunner' DATAFLOW_PROJECT = 'liquid-alloy-178307' Para apuntar a un fichero en storage el formato es: gs://nombre del bucket/direccion donde esta el archivo en ese bucket Ejemplo: gs://apro-verlar-staging-temp/tirado_202202.csv py beam4.py --input gs://apro-verlar-staging-temp/tirado_202202.csv --output gs://apro-verlar-staging-temp/tirado_202202_copia.csv --staging_location gs://apro-verlar/beam-test01/staging --temp_location gs://apro-verlar/beam-test01/temp --region us-central1 --network default --subnetwork regions/us-central1/subnetworks/default --runner DataflowRunner --project liquid-alloy-178307","title":"APACHE BEAM"},{"location":"beam/#apache-beam","text":"Maneja y transforma datos en masivo. Utiliza m\u00e1quinas externas para ello. Soporta batch (los datos tienen un fin) y streaming (los datos est\u00e1n entrando continuamente). Tutorial b\u00e1sico","title":"APACHE BEAM"},{"location":"beam/#iniciar","text":"Instalar SDK de Python Beam https://beam.apache.org/get-started/quickstart-py/ pip install apache-beam[gcp] Escribir c\u00f3digo en un archivo python importando apahe_beam Se puede ejecutar en local si es peque\u00f1o, sino hay que usar alguna herramienta (nosotros usaremos dataflow)","title":"INICIAR"},{"location":"beam/#conceptos-basicos","text":"","title":"Conceptos b\u00e1sicos"},{"location":"beam/#pipeline","text":"Un Pipeline es el proceso que queremos seguir. Esto inclue leer datos, transformarlos y escribir un output. Todos los programas de Beam tienen que crear un Pipeline. Adem\u00e1s, habr\u00e1 que especificar las opciones con las que queremos ejecutar el c\u00f3digo y d\u00f3nde queremos hacerlo (--runner). --runner DirectRuner #ejecutarlo en local --runner DataflowRunner #ejecutarlo en cloud","title":"Pipeline"},{"location":"beam/#pcollection","text":"Representa un conjunto de datos distribuido (no tiene orden). Puede ser acotada y que est\u00e9 fija en un archivo o infinita dado que entran datos continuamente. Your pipeline typically creates an initial PCollection by reading data from an external data source, but you can also create a PCollection from in-memory data within your driver program. From there, PCollections are the inputs and outputs for each step in your pipeline.","title":"PCollection"},{"location":"beam/#ptransform","text":"A PTransform represents a data processing operation, or a step, in your pipeline. Every PTransform takes one or more PCollection objects as input, performs a processing function that you provide on the elements of that PCollection, and produces zero or more output PCollection objects.","title":"PTransform"},{"location":"beam/#proceso","text":"Con el c\u00f3digo lo q hacemos es escribir el pipeline que luego se ejecutar\u00e1 en una m\u00e1quina (en nuestro caso Cloud Dataflow). Tembi\u00e9n podr\u00edamos ejecutarlo en local si la cantidad de datos es peque\u00f1a.","title":"Proceso"},{"location":"beam/#ejemplo-1","text":"Tomar\u00e1 datos de una fuente (tambi\u00e9n podr\u00edan ser m\u00e1s), realizaremos transformaciones sobre esos datos, y los escribiremos. ETL (Extract, Transform, Load). Tomaremos el Quijote lo leeremos, contaremos las palabras y mostraremos las 5 m\u00e1s repetidas. main.py: from typing import Tuple import apache_beam as beam import argparse from apache_beam import PCollection from apache_beam.options.pipeline_options import PipelineOptions def main(): parser = argparse.ArgumentParser(description=\"Nuestro primer pipeline\") parser.add_argument(\"--entrada\", help=\"Fichero de entrada\") parser.add_argument(\"--salida\", help=\"Fichero de salida\") def run_pipeline(custom_args, beam_args): entrada = custom_args.entrada salida = custom_args.salida opts = PipelineOptions(beam_args) with beam.Pipeline(options=opts) as p: lineas: PCollection[str] = p | \"Leemos entrada\" >> beam.io.ReadFromText(entrada) # \"En un lugar de La Mancha\" --> [\"En\", \"un\", ...], [...], [...] --> \"En\", \"un\", \"lugar\", .... palabras = lineas | \"Pasamos a palabras\" >> beam.FlatMap(lambda l: l.split()) contadas: PCollection[Tuple[str, int]] = limpiadas | \"Contamos\" >> beam.combiners.Count.PerElement() #\"En\" -> (\"En\", 17) # \"un\" -> (\"un\", 28) palabras_top_lista = contadas | \"Ranking\" >> beam.combiners.Top.Of(n_palabras, key=lambda kv: kv[1]) palabras_top = palabras_top_lista | \"Desenvuelve lista\" >> beam.FlatMap(lambda x: x) formateado: PCollection[str] = palabras_top | \"Formateamos\" >> beam.Map(lambda kv: \"%s,%d\" % (kv[0], kv[1])) formateado | \"Escribimos salida\" >> beam.io.WriteToText(salida) if __name__ == '__main__': main() Ejecutar en local: py main.py --entrada quijote.txt --salida salida.txt --runner DirectRunner","title":"Ejemplo 1"},{"location":"beam/#ejemplo-2","text":"Ejemplo b\u00e1sico de un pipeline: import apache_beam as beam from apache_beam import Map from apache_beam.io.textio import ReadFromText, WriteToText from apache_beam.coders.coders import Coder import argparse class LatinCoder(Coder): \"\"\"A coder used for reading and writing strings as Latin-1.\"\"\" def encode(self, value): return value.encode('latin-1') def decode(self, value): return value.decode('latin-1') def is_deterministic(self): return True def csv_to_dict(line): # TODO: realizar modificaciones a cada l\u00ednea return line if __name__ == '__main__': parser = argparse.ArgumentParser() # Argumentos necesarios para ejecutar el Pipeline. parser.add_argument('--runner', required=True) parser.add_argument('--project', required=True) parser.add_argument('--region', required=True) parser.add_argument('--staging_location', required=True) parser.add_argument('--temp_location', required=True) parser.add_argument('--network', required=True) parser.add_argument('--subnetwork', required=True) parser.add_argument('--job_name', required=True) parser.add_argument('--input', required=True) parser.add_argument('--output', required=True) known_args, pipeline_args = parser.parse_known_args() p = beam.Pipeline(runner=\"DataflowRunner\", argv=[\"--project\", known_args.project, \"--staging_location\", known_args.staging_location, \"--temp_location\", known_args.temp_location, \"--save_main_session\", \"True\", \"--region\", known_args.region, \"--network\", known_args.network, \"--subnetwork\", known_args.subnetwork, \"--job_name\", known_args.job_name ]) (p | 'Read' >> ReadFromText(file_pattern=known_args.input, skip_header_lines=1, coder=LatinCoder()) | 'Make Dic' >> Map(csv_to_dict) | 'Write' >> # TODO: a\u00f1adir c\u00f3digo para escribir el resultado utilizando 'WriteToText' ) p.run().wait_until_finish()","title":"Ejemplo 2"},{"location":"beam/#ejecutar-en-cloud","text":"","title":"Ejecutar en cloud"},{"location":"beam/#parametros-necesarios","text":"Explicaci\u00f3n de los par\u00e1metros parser = argparse.ArgumentParser() # Argumentos necesarios para ejecutar el Pipeline. parser.add_argument('--runner', required=True) parser.add_argument('--project', required=True) parser.add_argument('--region', required=True) parser.add_argument('--staging_location', required=True) parser.add_argument('--temp_location', required=True) parser.add_argument('--network', required=True) parser.add_argument('--subnetwork', required=True) parser.add_argument('--job_name', required=True) parser.add_argument('--input', required=True) parser.add_argument('--output', required=True) known_args, pipeline_args = parser.parse_known_args() p = beam.Pipeline(runner=\"DataflowRunner\", argv=[\"--project\", known_args.project, \"--staging_location\", known_args.staging_location, \"--temp_location\", known_args.temp_location, \"--save_main_session\", \"True\", \"--region\", known_args.region, \"--network\", known_args.network, \"--subnetwork\", known_args.subnetwork, \"--job_name\", known_args.job_name ]) Algunos par\u00e1metros por defecto para probar: STAGING_NAME = 'gs://apro-verlar/beam-test01/staging' TEMP_NAME = 'gs://apro-verlar/beam-test01/temp' REGION = 'us-central1' NETWORK = 'default' SUBNETWORK = 'regions/us-central1/subnetworks/default' RUNNER = 'DataflowRunner' DATAFLOW_PROJECT = 'liquid-alloy-178307' Para apuntar a un fichero en storage el formato es: gs://nombre del bucket/direccion donde esta el archivo en ese bucket Ejemplo: gs://apro-verlar-staging-temp/tirado_202202.csv py beam4.py --input gs://apro-verlar-staging-temp/tirado_202202.csv --output gs://apro-verlar-staging-temp/tirado_202202_copia.csv --staging_location gs://apro-verlar/beam-test01/staging --temp_location gs://apro-verlar/beam-test01/temp --region us-central1 --network default --subnetwork regions/us-central1/subnetworks/default --runner DataflowRunner --project liquid-alloy-178307","title":"Par\u00e1metros necesarios:"},{"location":"bigquery/","text":"BIGQUERY Video Docs Partitioning https://cloud.google.com/bigquery/docs/partitioned-tables Una tabla de partici\u00f3n es una tabla especial que se divide en segmentos, denominados particiones, que facilitan la administraci\u00f3n y la consulta de tus datos. Dividir una tabla grande en particiones m\u00e1s peque\u00f1as puede mejorar el rendimiento de la consulta. Adem\u00e1s, puedes controlar los costos si reduces la cantidad de bytes que lee una consulta. Clustering","title":"BIGQUERY"},{"location":"bigquery/#bigquery","text":"Video Docs","title":"BIGQUERY"},{"location":"bigquery/#partitioning","text":"https://cloud.google.com/bigquery/docs/partitioned-tables Una tabla de partici\u00f3n es una tabla especial que se divide en segmentos, denominados particiones, que facilitan la administraci\u00f3n y la consulta de tus datos. Dividir una tabla grande en particiones m\u00e1s peque\u00f1as puede mejorar el rendimiento de la consulta. Adem\u00e1s, puedes controlar los costos si reduces la cantidad de bytes que lee una consulta.","title":"Partitioning"},{"location":"bigquery/#clustering","text":"","title":"Clustering"},{"location":"django/","text":"DJANGO Django web INICIAR Tutorial cd <directory where I want to create the project> django-admin startproject myproject Esto crear\u00e1 un subdirectorio llamado myproject con la siguiente estructura: myproject/ manage.py myproject/ __init__.py settings.py urls.py asgi.py wsgi.py Crear aplicaciones en el proyecto Primero hay que entender la diferencia entre proyecto y aplicaci\u00f3n. Una aplicaci\u00f3n siempre ser\u00e1 parte de nuestro proyecto. El proyecto podr\u00e1 tener varias aplicaciones. Por ejemplo el proyecto de una tienda online puede tener la aplicaci\u00f3n de gestionar el panel de control, el del stock de almac\u00e9n, el de los pagos... Y estas aplicaciones podr\u00e1n ser reutilizadas en otros proyectos. Cada aplicaci\u00f3n en Django consiste en un paquete de Python que se estructura de una cierta manera por convenci\u00f3n. Para crear la primera app, colocarse en la carpeta donde est\u00e1 el archivo \u00b4manage.py\u00b4 y ejecutar: Crearemos la aplicaci\u00f3n llamada core py manage.py startapp core Se crear\u00e1 la carpeta core de la aplicaci\u00f3n con esta estructura: | myproject/ | core/ | migrations/ | __init__.py | __init__.py | admin.py | apps.py | models.py | tests.py | views.py | myproject/ | __init__.py | settings.py | urls.py | asgi.py | wsgi.py | manage.py Para decirle a Django que tenga en cuenta la aplicaci\u00f3n tenemos que cambiar el archivo myproject/settings.py : INSTALED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'core' ] Modelos Documentaci\u00f3n modelos Tendremos que crear un modelo para cada tabla que queramos crear en la base de datos a la que estemos conectados (por defecto SQLite3). Este modelo nos especificar\u00e1 los campos que tendr\u00e1 la tabla. from django.db import models from django.utils.translation import gettext_lazy as _ class Vehiculo(models.Model): TIPO_CHOICES = [ ('camion', 'Cami\u00f3n'), ('coche', 'Coche'), ('furgoneta', 'Furgoneta'), ] matricula = models.CharField(max_length=7, verbose_name=_('Matr\u00edcula')) description = models.CharField(max_length=100, verbose_name=_('Descripci\u00f3n')) precio = models.FloatField(verbose_name=_('Precio')) tipo = models.CharField(max_length=30, verbose_name=_('Eleccion'), choices=TIPO_CHOICES) class Meta: ordering = ['pk'] verbose_name = 'Vehiculo' verbose_name_plural = 'Vehiculos' Bases de datos relacionales Si queremos relacionar tablas entre ellas utilizaremos ForeignKey . ForeignKey_docs class Conductor(models.Model): nombre = models.CharField(max_length=100, verbose_name=_('Nombre')) vehiculo = models.ForeignKey(Vehiculo, on_delete=models.PROTECT, verbose_name=_('Veh\u00edculo'), related_name=' ') class Meta: verbose_name = 'Conductor' verbose_name_plural = 'Conductores' Cada conductor tendr\u00e1 un veh\u00edcuo asociado. Para acceder a los atributos del veh\u00edculo se utilizar\u00e1 __ (doble barra baja). Ejemplo: vehiculo__matricula Una vez creados los modelos tenedremos que hacer las migraciones para que la base de datos sepa c\u00f3mo son estos modelos. Primero crearemos los archivos de migraci\u00f3n: py manage.py makemigrations Despu\u00e9s Django leer\u00e1 esos archivos de migraci\u00f3n creados y har\u00e1 la migraci\u00f3n: py manage.py migrate !!! warning \"Warning.\" Cada vez que se modifica un modelo hay que hacer una migraci\u00f3n. Si no est\u00e1n todos los archivos migration Django no podr\u00e1 realizar la migraci\u00f3n. Views and Urls Cada view nos ense\u00f1ar\u00e1 algo de la base de datos y ser\u00e1 capaz de enviar estos datos como respuesta a un request del front. Hay diferentes tipos de views para ello (se explican m\u00e1s adelante). Para hacer la petici\u00f3n a cada una de las views se utilizan las url-s. URLS Para ello tenemos que configurar dos archivos: myproject/urls.py from django.conf.urls import include from django.contrib import admin from django.urls import path urlpatterns = [ path('core/', include('core.urls')), path('admin/', admin.site.urls), ] En este archivo se le especifica a Djago que mire en core/urls.py para ver qu\u00e9 urls tiene que utilizar para cada una de las views. core/urls.py from django.urls import path from core.views import * app_name = 'core' \"\"\" Dentro de la aplicaci\u00f3n ``core`` tendremos las siguientes URL-s. Algunas solamente sirven para mostrar la plantilla HTML y las otras son para obtener datos con el filtro deseado y mostrarlos. \"\"\" urlpatterns = [ path('conductores/', FlujosView.as_view(), name='conductores'), path('vehiculos/', AlertasView.as_view(), name='vehiculos') ] Hace la relaci\u00f3n entre las url-s de petici\u00f3n y la view encargada de dar respuesta a esta. VIEWS Este archivo estar\u00e1 en core/views.py . Hay diferentes tipos de views. Ejemplo: from django.http import HttpResponse def index(request): return HttpResponse(\"Hello, world. You're at the polls index.\") Documentaci\u00f3n de request response View para cargar una plantilla HTML Podremos crear documentos HTML y cargarlos mediante un TemplateView . Para ello colocaremos los archivos necesarios para el HTML en estos directorios: | myproject/ | core/ | migrations/ | 0001_initial.py | 0002_auto_20220609_1028.py | static/ | core/ | css/ | img/ | js/ | templates/ | index.html | vehiculos_page.html | __init__.py | admin.py | apps.py | models.py | tests.py | views.py | myproject/ | __init__.py | settings.py | urls.py | asgi.py | wsgi.py | manage.py Y ya podremos cargar las views: class indexView(TemplateView): template_name = 'core/index.html' class vehiculosView(TemplateView): template_name = 'core/vehiculos_page.html' De esta forma cuando llamemos a la url hostServer:port\\core\\vehiculos , se nos cargar\u00e1 el html vehiculos_page. En cambio, no podremos acceder al indexView porque no lo hemos especificado en el archivo urls.py . Views para filtrar y agregar datos Nos podemos apoyar en DjangoREST para esto. Ejemplo de filtro y agragaci\u00f3n (el ejemplo no sigue el modelo de vehiculo y conductor de antes): class cajasListAPIView(ListAPIView): \"\"\" Filtrar\u00e1 las fechas y el tipo de caja en cada caso y har\u00e1 la agrupaci\u00f3n para sumar cajas. \"\"\" queryset = Link.objects.all() filter_backends = [DjangoFilterBackend] filter_fields = { 'dia': [\"in\", \"exact\"], 'tipo_caja': [\"in\",\"exact\"] } def get(self, request, **kwargs): queryset = self.get_queryset() filter_queryset = self.filter_queryset(queryset) values = filter_queryset.values('dia','tipo_caja')\\ .annotate(n_cajas=Sum('cajas')) return Response(values) Podemos hacer este tipo de peticiones a esta view: `hostServer:port\\core\\cajas?dia__in=20220628,20220629&tipo_caja=3` Esta petici\u00f3n nos devolver\u00e1 un JSON donde por nos sumar\u00e1 la cantidad de cajas de tipo 3 del 28 de abril o 29 de abril. Carga de datos a heroku Primero hay que especificarle a Django que queremos trabajar con una base de datos de Heroku. ... La carga se har\u00e1 solo una vez de forma manual. py manage.py shell Una vez en shell ejecutamos el script destinado a cargar datos. La forma m\u00e1s facil de subir datos es creando una lista con objetos del modelo que queramos cargar, y cargar toda la lista a la vez. Por ejemplo, si tenemos todos los conductores (modelo Conductor) en un excel, primero tendremos que subir los veh\u00edculos (porque los conductores tienen un vehiculo como atributo) y despu\u00e9s coger el atributo que queramos para el conductor: import pandas as pd from core.models import Conductor, Vehiculo df_vehiculos = pd.read_excel(\"vehiculos.xlsx\") df_conductores = pd.read_excel(\"conductores.xlsx\") # Creamos una lista llena de objetos ``Vehiculo`` l_vehiculos = [Vehiculo( matricula=row['matricula'], description=row['description'], precio=row['precio'], tipo=row['tipo'], ) for i, row in df_conductores.iterrows()] # Subimos la lista a Heroku Vehiculo.objects.bulk_create(l_vehiculos) # Creamos una lista llena de objetos ``Conductor`` l_conductores = [Conductor( nombre=row['nombre'], matricula=Nodo.objects.get(matricula=row['matricula']), ) for i, row in df_conductores.iterrows()] # Subimos la lista a Heroku Conductor.objects.bulk_create(l_conductores) Django admin python manage.py createsuperuser Username: admin Email address: admin@example.com Password: ********** Password (again): ********* Superuser created successfully. En core.admin.py from django.contrib import admin from .models import Vehiculo admin.site.register(Vehiculo) Ahora en http://127.0.0.1:8000/admin ya podremos acceder y modificar campos. Interacci\u00f3n con el front (javascript) Para hacer peticiones a Django tenemos que utilizar URL-s. Ejemplo: var url = \"vehiculos/\" var response = await fetch(url) response = await answer.json() Esto nos devolver\u00e1 la respuesta en formato JSON. Si quisieramos hacer alg\u00fan cambio en alguno de los atributos podemos iterar sobre ese JSON. Por ejemplo queremos rebajar todos los veh\u00edculos un 20%: var vehiculos = response.map(function(vehc){ vehc.precio = vehc.precio * 0.8 }); Tambi\u00e9n podremos hacer llamadas simult\u00e1neas a diferentes url-s: var urls = ['vehiculos\\', 'conductores\\']; var promises = urls.map(url => fetch(url).then(y => y.text())); Promise.all(promises).then(response => { console.log(response) }); Ese response nos ense\u00f1ar\u00e1 las diferentes respuestas de Django.","title":"DJANGO"},{"location":"django/#django","text":"Django web","title":"DJANGO"},{"location":"django/#iniciar","text":"Tutorial cd <directory where I want to create the project> django-admin startproject myproject Esto crear\u00e1 un subdirectorio llamado myproject con la siguiente estructura: myproject/ manage.py myproject/ __init__.py settings.py urls.py asgi.py wsgi.py","title":"INICIAR"},{"location":"django/#crear-aplicaciones-en-el-proyecto","text":"Primero hay que entender la diferencia entre proyecto y aplicaci\u00f3n. Una aplicaci\u00f3n siempre ser\u00e1 parte de nuestro proyecto. El proyecto podr\u00e1 tener varias aplicaciones. Por ejemplo el proyecto de una tienda online puede tener la aplicaci\u00f3n de gestionar el panel de control, el del stock de almac\u00e9n, el de los pagos... Y estas aplicaciones podr\u00e1n ser reutilizadas en otros proyectos. Cada aplicaci\u00f3n en Django consiste en un paquete de Python que se estructura de una cierta manera por convenci\u00f3n. Para crear la primera app, colocarse en la carpeta donde est\u00e1 el archivo \u00b4manage.py\u00b4 y ejecutar: Crearemos la aplicaci\u00f3n llamada core py manage.py startapp core Se crear\u00e1 la carpeta core de la aplicaci\u00f3n con esta estructura: | myproject/ | core/ | migrations/ | __init__.py | __init__.py | admin.py | apps.py | models.py | tests.py | views.py | myproject/ | __init__.py | settings.py | urls.py | asgi.py | wsgi.py | manage.py Para decirle a Django que tenga en cuenta la aplicaci\u00f3n tenemos que cambiar el archivo myproject/settings.py : INSTALED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'core' ]","title":"Crear aplicaciones en el proyecto"},{"location":"django/#modelos","text":"Documentaci\u00f3n modelos Tendremos que crear un modelo para cada tabla que queramos crear en la base de datos a la que estemos conectados (por defecto SQLite3). Este modelo nos especificar\u00e1 los campos que tendr\u00e1 la tabla. from django.db import models from django.utils.translation import gettext_lazy as _ class Vehiculo(models.Model): TIPO_CHOICES = [ ('camion', 'Cami\u00f3n'), ('coche', 'Coche'), ('furgoneta', 'Furgoneta'), ] matricula = models.CharField(max_length=7, verbose_name=_('Matr\u00edcula')) description = models.CharField(max_length=100, verbose_name=_('Descripci\u00f3n')) precio = models.FloatField(verbose_name=_('Precio')) tipo = models.CharField(max_length=30, verbose_name=_('Eleccion'), choices=TIPO_CHOICES) class Meta: ordering = ['pk'] verbose_name = 'Vehiculo' verbose_name_plural = 'Vehiculos'","title":"Modelos"},{"location":"django/#bases-de-datos-relacionales","text":"Si queremos relacionar tablas entre ellas utilizaremos ForeignKey . ForeignKey_docs class Conductor(models.Model): nombre = models.CharField(max_length=100, verbose_name=_('Nombre')) vehiculo = models.ForeignKey(Vehiculo, on_delete=models.PROTECT, verbose_name=_('Veh\u00edculo'), related_name=' ') class Meta: verbose_name = 'Conductor' verbose_name_plural = 'Conductores' Cada conductor tendr\u00e1 un veh\u00edcuo asociado. Para acceder a los atributos del veh\u00edculo se utilizar\u00e1 __ (doble barra baja). Ejemplo: vehiculo__matricula Una vez creados los modelos tenedremos que hacer las migraciones para que la base de datos sepa c\u00f3mo son estos modelos. Primero crearemos los archivos de migraci\u00f3n: py manage.py makemigrations Despu\u00e9s Django leer\u00e1 esos archivos de migraci\u00f3n creados y har\u00e1 la migraci\u00f3n: py manage.py migrate !!! warning \"Warning.\" Cada vez que se modifica un modelo hay que hacer una migraci\u00f3n. Si no est\u00e1n todos los archivos migration Django no podr\u00e1 realizar la migraci\u00f3n.","title":"Bases de datos relacionales"},{"location":"django/#views-and-urls","text":"Cada view nos ense\u00f1ar\u00e1 algo de la base de datos y ser\u00e1 capaz de enviar estos datos como respuesta a un request del front. Hay diferentes tipos de views para ello (se explican m\u00e1s adelante). Para hacer la petici\u00f3n a cada una de las views se utilizan las url-s.","title":"Views and Urls"},{"location":"django/#urls","text":"Para ello tenemos que configurar dos archivos: myproject/urls.py from django.conf.urls import include from django.contrib import admin from django.urls import path urlpatterns = [ path('core/', include('core.urls')), path('admin/', admin.site.urls), ] En este archivo se le especifica a Djago que mire en core/urls.py para ver qu\u00e9 urls tiene que utilizar para cada una de las views. core/urls.py from django.urls import path from core.views import * app_name = 'core' \"\"\" Dentro de la aplicaci\u00f3n ``core`` tendremos las siguientes URL-s. Algunas solamente sirven para mostrar la plantilla HTML y las otras son para obtener datos con el filtro deseado y mostrarlos. \"\"\" urlpatterns = [ path('conductores/', FlujosView.as_view(), name='conductores'), path('vehiculos/', AlertasView.as_view(), name='vehiculos') ] Hace la relaci\u00f3n entre las url-s de petici\u00f3n y la view encargada de dar respuesta a esta.","title":"URLS"},{"location":"django/#views","text":"Este archivo estar\u00e1 en core/views.py . Hay diferentes tipos de views. Ejemplo: from django.http import HttpResponse def index(request): return HttpResponse(\"Hello, world. You're at the polls index.\") Documentaci\u00f3n de request response","title":"VIEWS"},{"location":"django/#view-para-cargar-una-plantilla-html","text":"Podremos crear documentos HTML y cargarlos mediante un TemplateView . Para ello colocaremos los archivos necesarios para el HTML en estos directorios: | myproject/ | core/ | migrations/ | 0001_initial.py | 0002_auto_20220609_1028.py | static/ | core/ | css/ | img/ | js/ | templates/ | index.html | vehiculos_page.html | __init__.py | admin.py | apps.py | models.py | tests.py | views.py | myproject/ | __init__.py | settings.py | urls.py | asgi.py | wsgi.py | manage.py Y ya podremos cargar las views: class indexView(TemplateView): template_name = 'core/index.html' class vehiculosView(TemplateView): template_name = 'core/vehiculos_page.html' De esta forma cuando llamemos a la url hostServer:port\\core\\vehiculos , se nos cargar\u00e1 el html vehiculos_page. En cambio, no podremos acceder al indexView porque no lo hemos especificado en el archivo urls.py .","title":"View para cargar una plantilla HTML"},{"location":"django/#views-para-filtrar-y-agregar-datos","text":"Nos podemos apoyar en DjangoREST para esto. Ejemplo de filtro y agragaci\u00f3n (el ejemplo no sigue el modelo de vehiculo y conductor de antes): class cajasListAPIView(ListAPIView): \"\"\" Filtrar\u00e1 las fechas y el tipo de caja en cada caso y har\u00e1 la agrupaci\u00f3n para sumar cajas. \"\"\" queryset = Link.objects.all() filter_backends = [DjangoFilterBackend] filter_fields = { 'dia': [\"in\", \"exact\"], 'tipo_caja': [\"in\",\"exact\"] } def get(self, request, **kwargs): queryset = self.get_queryset() filter_queryset = self.filter_queryset(queryset) values = filter_queryset.values('dia','tipo_caja')\\ .annotate(n_cajas=Sum('cajas')) return Response(values) Podemos hacer este tipo de peticiones a esta view: `hostServer:port\\core\\cajas?dia__in=20220628,20220629&tipo_caja=3` Esta petici\u00f3n nos devolver\u00e1 un JSON donde por nos sumar\u00e1 la cantidad de cajas de tipo 3 del 28 de abril o 29 de abril.","title":"Views para filtrar y agregar datos"},{"location":"django/#carga-de-datos-a-heroku","text":"Primero hay que especificarle a Django que queremos trabajar con una base de datos de Heroku. ... La carga se har\u00e1 solo una vez de forma manual. py manage.py shell Una vez en shell ejecutamos el script destinado a cargar datos. La forma m\u00e1s facil de subir datos es creando una lista con objetos del modelo que queramos cargar, y cargar toda la lista a la vez. Por ejemplo, si tenemos todos los conductores (modelo Conductor) en un excel, primero tendremos que subir los veh\u00edculos (porque los conductores tienen un vehiculo como atributo) y despu\u00e9s coger el atributo que queramos para el conductor: import pandas as pd from core.models import Conductor, Vehiculo df_vehiculos = pd.read_excel(\"vehiculos.xlsx\") df_conductores = pd.read_excel(\"conductores.xlsx\") # Creamos una lista llena de objetos ``Vehiculo`` l_vehiculos = [Vehiculo( matricula=row['matricula'], description=row['description'], precio=row['precio'], tipo=row['tipo'], ) for i, row in df_conductores.iterrows()] # Subimos la lista a Heroku Vehiculo.objects.bulk_create(l_vehiculos) # Creamos una lista llena de objetos ``Conductor`` l_conductores = [Conductor( nombre=row['nombre'], matricula=Nodo.objects.get(matricula=row['matricula']), ) for i, row in df_conductores.iterrows()] # Subimos la lista a Heroku Conductor.objects.bulk_create(l_conductores)","title":"Carga de datos a heroku"},{"location":"django/#django-admin","text":"python manage.py createsuperuser Username: admin Email address: admin@example.com Password: ********** Password (again): ********* Superuser created successfully. En core.admin.py from django.contrib import admin from .models import Vehiculo admin.site.register(Vehiculo) Ahora en http://127.0.0.1:8000/admin ya podremos acceder y modificar campos.","title":"Django admin"},{"location":"django/#interaccion-con-el-front-javascript","text":"Para hacer peticiones a Django tenemos que utilizar URL-s. Ejemplo: var url = \"vehiculos/\" var response = await fetch(url) response = await answer.json() Esto nos devolver\u00e1 la respuesta en formato JSON. Si quisieramos hacer alg\u00fan cambio en alguno de los atributos podemos iterar sobre ese JSON. Por ejemplo queremos rebajar todos los veh\u00edculos un 20%: var vehiculos = response.map(function(vehc){ vehc.precio = vehc.precio * 0.8 }); Tambi\u00e9n podremos hacer llamadas simult\u00e1neas a diferentes url-s: var urls = ['vehiculos\\', 'conductores\\']; var promises = urls.map(url => fetch(url).then(y => y.text())); Promise.all(promises).then(response => { console.log(response) }); Ese response nos ense\u00f1ar\u00e1 las diferentes respuestas de Django.","title":"Interacci\u00f3n con el front (javascript)"},{"location":"git/","text":"GIT Posibles errores the tip of your current branch is behind its remote counterpart means that there have been changes on the remote branch that you don\u2019t have locally. and git tells you import new changes from REMOTE and merge it with your code and then push it to remote. git push -f <repository> <branch>","title":"GIT"},{"location":"git/#git","text":"","title":"GIT"},{"location":"git/#posibles-errores","text":"the tip of your current branch is behind its remote counterpart means that there have been changes on the remote branch that you don\u2019t have locally. and git tells you import new changes from REMOTE and merge it with your code and then push it to remote. git push -f <repository> <branch>","title":"Posibles errores"},{"location":"links/","text":"LINKS O INFORMACI\u00d3N","title":"LINKS O INFORMACI\u00d3N"},{"location":"links/#links-o-informacion","text":"","title":"LINKS O INFORMACI\u00d3N"},{"location":"pandas/","text":"LIBRER\u00cdA PANDAS DE PYTHON INICIO pip install wheel pip install pandas pip install openpyxl C\u00d3DIGO B\u00c1SICO import pandas as pd df = pd.read_excel('nombre_de_archivo.xlsx', sheet_name=nombre de hoja)","title":"PANDAS"},{"location":"pandas/#libreria-pandas-de-python","text":"","title":"LIBRER\u00cdA PANDAS DE PYTHON"},{"location":"pandas/#inicio","text":"pip install wheel pip install pandas pip install openpyxl","title":"INICIO"},{"location":"pandas/#codigo-basico","text":"import pandas as pd df = pd.read_excel('nombre_de_archivo.xlsx', sheet_name=nombre de hoja)","title":"C\u00d3DIGO B\u00c1SICO"},{"location":"prophet/","text":"PROPHET Documentaci\u00f3n python INSTALACI\u00d3N Documentaci\u00f3n: pip install pystan==2.19.1.1 pip install prophet Si diera error: pip install localpip localpip install fbprophet Para importar: from fbprophet install Prophet AN\u00c1LISIS DEL C\u00d3DIGO \"N\u00daCLEO\" C\u00f3digo Predicci\u00f3n de ventas o necesidad de art\u00edculos indistintivamente, ya que una variable est\u00e1 relacionada con la otra. El objetivo ser\u00e1 crear una tabla prediccion centro . 1) Se cargan los datos de ventas: df 2) Cargamos informaci\u00f3n de calendario y centros: calendario y Maes_cen 3) create_everyday_data() 4) filter_predict_data() Se filtra: Cogemos los datos a partir de 2017 Cogemos los centros que hayan tenido ventas alg\u00fan d\u00eda de los \u00faltimos 15 Cogemos los centros que tengan un m\u00ednimo hist\u00f3rico de 7 d\u00edas 5) Se cargan los datos del 2017-2018 manualmente ( manual_2017_2018_calendar ) y add_calendar_info : A\u00f1adimos la informaci\u00f3n del calendario al DataFrame principal. Generamos una nueva columna historico_anual para indicar si tiene m\u00e1s de un a\u00f1o de hist\u00f3rico o no 6) add_factor_seasonalities A\u00f1adimos extepciones de julio, agosto, y septiembre (hasta el 15) y COVID. En nuevas columnas que se llaman: season_7 , season_8 , season_9 , alerta_covid19 , confinamiento_covid19 , toque_de_queda_covid19 7) add_regressor Variables regresoras: Bajada gasolineras Recuperaci\u00f3n gasolineras Cierre Bilbao 8) delete_outliers Periodo COVID Huelga 8-M Nevada 2018/02/28 y 2018/03/01 Outliers por per\u00edmetro: Euskadi Vegalsa Catalu\u00f1a Outliers extremas. Nuestra variable x tendr\u00e1 que cumplir: mediana\u00b70,05<=x<=mediana\u00b720 Las dem\u00e1s se descartan. A PARTIR DE AHORA EMPEZAREMOS A APLICAR LAS FUNCIONES POR CADA CENTRO: 1) correct_calendar_center() , delete_center_outliers() 2) Se tienen en cuenta los siguientes d\u00edas seg\u00fan de d\u00f3nde es el centro y siempre que tengamos m\u00e1s de cuatro meses de hist\u00f3rico. Sino se eliminan los d\u00edas de apertura de la media. Ahora mismo lo m\u00e1ximo que est\u00e1 afinado es por tipo de centro (hiper, super, gasolinera) y per\u00edmetro (norte,sur,caprabo,vegalsa,baleares) D\u00eda sin IVA Vales D\u00eda sin IVA en alimentaci\u00f3n y fresco D\u00eda sin IVA electro black friday (s\u00f3lo en hiper) Promoci\u00f3n doble Navidad Nochevieja Jueves Santo 24 y 31 de diciembre Fiestas nacionales Festivos Festivos nacionales com Reyes (en panader\u00eda) San Jos\u00e9 (en charcuter\u00eda) 3) De todo lo anterior se crea una tabla center con fechas y ventas correspondientes a \u00e9stas. m.fit(center) future = m.make_future_dataframe(periods=DIAS_PREDICCION) add_future_regressor() add_future_factor_seasonalities() forecast = m.predict(future) Al final, se crea una tabla predicciones_centro uniendo todas las predicciones que hemos hecho para cada centro. EJECUCI\u00d3N prophet.py [-h] --yhat_name YHAT_NAME --load_data_query LOAD_DATA_QUERY --table_name TABLE_NAME","title":"PROPHET"},{"location":"prophet/#prophet","text":"Documentaci\u00f3n python","title":"PROPHET"},{"location":"prophet/#instalacion","text":"Documentaci\u00f3n: pip install pystan==2.19.1.1 pip install prophet Si diera error: pip install localpip localpip install fbprophet Para importar: from fbprophet install Prophet","title":"INSTALACI\u00d3N"},{"location":"prophet/#analisis-del-codigo-nucleo","text":"C\u00f3digo Predicci\u00f3n de ventas o necesidad de art\u00edculos indistintivamente, ya que una variable est\u00e1 relacionada con la otra. El objetivo ser\u00e1 crear una tabla prediccion centro . 1) Se cargan los datos de ventas: df 2) Cargamos informaci\u00f3n de calendario y centros: calendario y Maes_cen 3) create_everyday_data() 4) filter_predict_data() Se filtra: Cogemos los datos a partir de 2017 Cogemos los centros que hayan tenido ventas alg\u00fan d\u00eda de los \u00faltimos 15 Cogemos los centros que tengan un m\u00ednimo hist\u00f3rico de 7 d\u00edas 5) Se cargan los datos del 2017-2018 manualmente ( manual_2017_2018_calendar ) y add_calendar_info : A\u00f1adimos la informaci\u00f3n del calendario al DataFrame principal. Generamos una nueva columna historico_anual para indicar si tiene m\u00e1s de un a\u00f1o de hist\u00f3rico o no 6) add_factor_seasonalities A\u00f1adimos extepciones de julio, agosto, y septiembre (hasta el 15) y COVID. En nuevas columnas que se llaman: season_7 , season_8 , season_9 , alerta_covid19 , confinamiento_covid19 , toque_de_queda_covid19 7) add_regressor Variables regresoras: Bajada gasolineras Recuperaci\u00f3n gasolineras Cierre Bilbao 8) delete_outliers Periodo COVID Huelga 8-M Nevada 2018/02/28 y 2018/03/01 Outliers por per\u00edmetro: Euskadi Vegalsa Catalu\u00f1a Outliers extremas. Nuestra variable x tendr\u00e1 que cumplir: mediana\u00b70,05<=x<=mediana\u00b720 Las dem\u00e1s se descartan. A PARTIR DE AHORA EMPEZAREMOS A APLICAR LAS FUNCIONES POR CADA CENTRO: 1) correct_calendar_center() , delete_center_outliers() 2) Se tienen en cuenta los siguientes d\u00edas seg\u00fan de d\u00f3nde es el centro y siempre que tengamos m\u00e1s de cuatro meses de hist\u00f3rico. Sino se eliminan los d\u00edas de apertura de la media. Ahora mismo lo m\u00e1ximo que est\u00e1 afinado es por tipo de centro (hiper, super, gasolinera) y per\u00edmetro (norte,sur,caprabo,vegalsa,baleares) D\u00eda sin IVA Vales D\u00eda sin IVA en alimentaci\u00f3n y fresco D\u00eda sin IVA electro black friday (s\u00f3lo en hiper) Promoci\u00f3n doble Navidad Nochevieja Jueves Santo 24 y 31 de diciembre Fiestas nacionales Festivos Festivos nacionales com Reyes (en panader\u00eda) San Jos\u00e9 (en charcuter\u00eda) 3) De todo lo anterior se crea una tabla center con fechas y ventas correspondientes a \u00e9stas. m.fit(center) future = m.make_future_dataframe(periods=DIAS_PREDICCION) add_future_regressor() add_future_factor_seasonalities() forecast = m.predict(future) Al final, se crea una tabla predicciones_centro uniendo todas las predicciones que hemos hecho para cada centro.","title":"AN\u00c1LISIS DEL C\u00d3DIGO \"N\u00daCLEO\""},{"location":"prophet/#ejecucion","text":"prophet.py [-h] --yhat_name YHAT_NAME --load_data_query LOAD_DATA_QUERY --table_name TABLE_NAME","title":"EJECUCI\u00d3N"},{"location":"redis/","text":"REDIS redis-py https://try.redis.io/ BASIC DATA STRUCTURES Keys and Expiration Documentation Keys are the primary way to access data values. Hay 16 databases, por defecto siempre se utilizar\u00e1 database0. EJEMPLO (por convenci\u00f3n, pero se puede hacer de otra forma): \"user:1000:followers\" user: nombre del objeto 1000: identificador \u00fanico de la instancia followers: objeto compuesto SET Para asignar un valor a una llave. SET customer:1000 \"fred\" En nuestra llave (cliente de ID 1000) customer:1000 estar\u00e1 guardado el valor \"fred\" Si hacemos SET a una llave que no existe la crearemos. si no queremos que esto suceda podemos utilizar NX SET provider:100 \"freddy\" NX Nos retornar\u00e1 OK si no existe y nos devolver\u00e1 NIL si existe. GET Para obtener el valor que tiene guardada la llave customer:1000 GET customer:1000 => fred En cambio, si queremos que la llave exista antes de hacer SET utilizaremos XX de la misma manera. KEYS and SCAN KEYS itera sobre todas las llaves, para ver si existe la que le hemos pedido. Si la base de datos es grande podr\u00eda tardar demasiado. SCAN itera usando un cursor y devuelve un espacio de referencia. Est\u00e1 bien para usarlo en producci\u00f3n. EJEMPLOS: KEYS Todos los clientes cuyo ID empiezan con 1: keys customer:1* => customer:1500 customer:1000 SCAN SCAN slot [MATCH pattern][COUNT count] scan 0 MATCH customer: 1* => 1) 14336 2) empty list or set scan 14336 MATCH customer:1* => 1) 14848 2) empty list or set scan 14848 MATCH customer:1* COUNT 10000 => 1) 1229 2) 1)customer:1500 2)customer:1000 scan 1229 MATCH customer:1* => 1) 0 2) empty list or set Cuando aparece 0 significa que no hay nada m\u00e1s sobre lo que iterar. ... Borrar llaves DEL Para borrar una llave y el valor asociado a ella o desvincular su llave con el valor. DEL key[key...] UNLINK key[key] EXISTS EXISTS server:name => 1 (existe) EXISTS server:blabla => 0 (no existe) Es una base de datos que funciona por llave-valor. Un valor siempre se guardar\u00e1 dentro de una llave y la \u00fanica forma de obtener dicho valor es tener la llave. Se explicar\u00e1n los comandos m\u00e1s b\u00e1sicos a continuaci\u00f3n, pero la lista completa esta aqu\u00ed INCR y INCRBY Incrementan el valor de una llave dada. INCR lo incrementa por 1 y INCRBY lo incrementa por el valor que nosotros le proporcionemos. Son operaciones at\u00f3micas. SET connections 10 INCR connections GET connections => 11 SET connections 10 INCRBY connections 100 GET connections => 110 DECR y DECRBY Decrementan el valor de una llave dada. INCR lo incrementa por 1 y INCRBY lo incrementa por el valor que nosotros le proporcionemos. EXPIRE y TTL Para decir que una llave tiene que existir solo para un tiempo especificado en segundos. SET resource:lock \"Redis Demo\" EXPIRE resource:lock 120 Con TTL podremos mirar cuento le queda a una llave: TTL resource:lock => 113 Si TTL es -1 significa que esa llave es para siempre. LISTAS PYTHON redis-py library pip install redis Connect to redis: import os import pytest import redis USERNAME = os.environ.get('REDISOLAR_REDIS_USERNAME') PASSWORD = os.environ.get('REDISOLAR_REDIS_PASSWORD') @pytest.fixture def redis_connection(app): client_kwargs = { \"host\": app.config['REDIS_HOST'], \"port\": app.config['REDIS_PORT'], \"decode_responses\": True } if USERNAME: client_kwargs[\"username\"] = USERNAME if PASSWORD: client_kwargs[\"password\"] = PASSWORD yield redis.Redis(**client_kwargs) def test_say_hello(redis_connection): result = redis_connection.set(\"hello\", \"world\") value = redis_connection.get(\"hello\") assert result is True assert value == \"world\" Redis Clients Gestiona conexiones, implementa el protocolo redis (RESP) y nos deja usar un lenguaje sencillo (GET, SET, INCR...). Walrus redis-py Utilizaremos redis-py como nuestra librer\u00eda cliente. C\u00f3mo conectarse a redis utilizando redis-py import redis from redis.sentinel import Sentinel from rediscluster import RedisCluster def connection_examples(): # Connect to a standard Redis deployment. client = redis.Redis(\"localhost\", port=6379, decode_responses=True, max_connections=20) # Read and write through the client. client.set(\"foo\", \"bar\") client.get(\"foo\") Basic operations Redys type > Python type: String > bytes or string List > list of bytes or strings Set > set of bytes or strings Hash > dictionary PLANETS = [ \"Mercury\", \"Mercury\", \"Venus\", \"Earth\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\" ] EARTH_KEY = \"earth\" ## LISTS def test_redis_list(redis, key_schema): key = key_schema.planets_list_key() assert len(PLANETS) == 11 # Add all test planets to a Redis list result = redis.rpush(key, *PLANETS) # Check that the length of the list in Redis is the same assert result == len(PLANETS) # Get the planets from the list # Note: LRANGE is an O(n) command. Be careful running this command # with large lists. planets = redis.lrange(key, 0, -1) assert planets == PLANETS # Remove the elements that we know are duplicates # Note: O(n) operation. redis.lrem(key, 1, \"Mercury\") redis.lrem(key, 1, \"Earth\") planet = redis.rpop(key) assert planet == \"Pluto\" assert redis.llen(key) == 8 ## SETS def test_redis_set(redis, key_schema): key = key_schema.planets_set_key() # Add planets to a Redis set redis.sadd(key, *PLANETS) # Return the cardinality of the set assert redis.scard(key) == 9 # Fetch all values from the set # Note: SMEMBERS is an O(n) command. Be careful running this command # with high-cardinality sets. Consider SSCAN as an alternative. assert redis.smembers(key) == set(PLANETS) # Pluto is, of course, no longer a first-class planet. Remove it. response = redis.srem(key, \"Pluto\") assert response == 1 # Now we have 8 planets, as expected. assert redis.scard(key) == 8 ## HASHES def test_redis_hash(redis): earth_properties = { \"diameter_km\": \"12756\", \"day_length_hrs\": \"24\", \"mean_temp_c\": \"15\", \"moon_count\": \"1\" } # Set the fields of the hash. redis.hset(EARTH_KEY, mapping=earth_properties) # Get the hash we just created back from Redis. stored_properties = redis.hgetall(EARTH_KEY) assert stored_properties == earth_properties # Test that we can get a single property. assert redis.hget(EARTH_KEY, \"diameter_km\") == earth_properties[\"diameter_km\"] CUIDADO!! Algunos m\u00e9todos pueden ser lentos. Para muchos datos, hay que utilizar los m\u00e9todos de la familia SCAN. DAOs Get started with redis cloud: Create a subscription and a database. To connect with the database we need: public endpoint for the host and port password for the authorization First code: import redis redis = redis.Redis( host='redis-16018.c228.us-central1-1.gce.cloud.redislabs.com', port='16018', password='AAa6DTbT7c0kYoZuVDM3mLTrEuGMinDt') redis.set('mykey', 'Hello from Python!') value = redis.get('mykey') print(value) redis.zadd('vehicles', {'car': 0}) redis.zadd('vehicles', {'bike': 0}) vehicles = redis.zrange('vehicles', 0, -1) print(vehicles) Output: b'Hello from Python!' [b'bike', b'car'] REDISEARCH Tutorial Aggregations tutorial RediSearch en python Redisearch es una herramienta de b\u00fasqueda. pip install redisearch Crear una instancia cliente. tutorial from redisearch import Client client = Client(\"my-index\") Client Python API from redis import ResponseError from redisearch import Client, IndexDefinition, TextField SCHEMA = ( TextField(\"title\", weight=5.0), TextField(\"body\") ) client = Client(\"my-index\") definition = IndexDefinition(prefix=['blog:']) try: client.info() except ResponseError: # Index does not exist. We need to create it! client.create_index(SCHEMA, definition=definition) from redisearch import Client, TextField, NumericField, Query # Creating a client with a given index name client = Client('myIndex') # Creating the index definition and schema client.create_index([TextField('title', weight=5.0), TextField('body')]) # Indexing a document client.add_document('doc1', title = 'RediSearch', body = 'Redisearch implements a search engine on top of redis') # Simple search res = client.search(\"search engine\") # the result has the total number of results, and a list of documents print res.total # \"1\" print res.docs[0].title # Searching with snippets res = client.search(\"search engine\", snippet_sizes = {'body': 50}) # Searching with complex parameters: q = Query(\"search engine\").verbatim().no_content().paging(0,5) res = client.search(q) QUERYING Finding exact string matches REDIS OM GitHub Yotube tutorial Youtube tutorial Curso de redis EJEMPLO: Vamos a montar un refugio de animales y tenemos un csv con los siguientes campos: name, species, age, weight, sex, fee, children, other_animals, description . Creamos un Redis Model en OM: from typing import Text from redis_om import (Field, HashModel) class Adoptable (HashModel): name: str = Field(index=True) species: str = Field(index=True) age: str = Field(index=True) weight: float = Field(index=True) sex: str = Field(index=True) fee: str = Field(index=True) children: str = Field(index=True) other_animals: str = Field(index=True) description: str = Field(index=True, full_text_search=True) #Le estamos diciendo a redis c\u00f3mo queremos que lo indexe, ya que queremos ser capaces de hacer un 'full text search' (utilizar\u00e1 un query de redisearch) Lo que estamos modelando se guardar\u00e1 como un hash en redis. Le decimos los campos (Field) que queremos que guarde y que queremos que los indexe. Lo indexar\u00e1 con redisearch y crear\u00e1 un \u00edndice para mantenimiento y b\u00fasqueda. De esta forma, cuando creemos instancias, client gestionar\u00e1 esto volviendolo a guardar en redis. Cada animal se guardar\u00e1 asociado a un valor (tal y como funciona redis, con clave-valor), pero nosostros queremos ser capaces de hacer b\u00fasquedas dependiendo de alg\u00fan campo (ej. queremos un perro que pese menos de 20kg). Reisearch nos ayudar\u00e1 en esto. Si quisieramos modelar algo con campos anidados (que no sea clave-valor como en este caso) podr\u00edamos utilizar el modelado JSON de redis. Ambos son modelos pedantic (una librer\u00eda de validaci\u00f3n de python) por lo que podemos a\u00f1adir criterios adicionales (que un campo sea un email, que un int est\u00e9 entre valores que especifiquemos...). Cargar datos en redis Creamos una base de datos en redis a partir de un CSV: import csv from adoptable import Adoptable from redis_om impor Migrator with open['animal_data.csv'] as csv_file: animail_reader = csv.DictReader(csv_file) for animal in animal_reader: adoptable = Adoptable(**animal) print(f\"{animal[name]} has pk = {adoptable.pk}\") # pk lo crea redis OM como clave primaria adoptable.save() # Hacer la migraci\u00f3n de datos Migrator().run() Con pk estamos creando claves que identifiquen a cada animal como \u00fanico localmente antes de guardarlo en redis. Con Migrator.run() activamos algo parecido a \"un detector de cambios\". As\u00ed todo lo que hagamos estar\u00e1 registrado siempre. Podemos descargarnos RedisInsight para interactuar con nuestra base de datos. Es una interfaz donde podemos ver y gestionar nuestros datos f\u00e1cilmente. Querys from adoptable import Adoptable def show_result(results): for adoptable in results: print(adoptable) print(\"\") def find_by_name(): print(\"find by name: \") return Adoptable.find(Adoptable.name == \"Luna\").all() def find_male_dogs(): print(\"find male dogs: \") return Adoptable.find((Adoptable.species == \"dog\") & (Adoptable.sex == \"m\") ) def find_dogs_in_age_range(): print(\"find dogs in ages range: \") return Adoptable.find((Adoptable.species == \"dog\") & (Adoptable.age < 8) & (Adoptable.age < 11) ).sort_by(\"age\") def find_cats_good_with_children(): print(\"find cats good with children: \") return Adoptable.find((Adoptable.species == \"cat\") & (Adoptable.children == \"y\") & (Adoptable.description % \"play\") & ~(Adoptable.description % \"anxious\") & ~(Adoptable.description % \"nervous\") ) show_results(find_by_name()) En la \u00faltima funci\u00f3n nos basamos en redisearch para las b\u00fasquedas. Si queremos buscar m\u00e1s all\u00e1 de valores concretos, nos podemos fijar en la descripci\u00f3n. Al buscar con % lo que hacemos es buscar una palabra parecida a la que estamos buscando, incluyendo tambi\u00e9n esa palabra. En cambio, con el s\u00edmbolo ~ lo que hacemos es excluir esa palabra y parecidas de nuestra b\u00fasqueda. REDIS_OM y REDISEARCH Tutorial redisearch Aggregations tutorial redisearch RediSearch en python GitHub Redis_OM Yotube tutorial Youtube tutorial Curso de redis oficial Vamos a manejar una base de datos con los siguientes campos: firstName,lastName,salary,department,isAdmin Iniciar Redis en Windows Tres opciones: 1) WSL DE LINUX 2) CON DOCKER Instalar docker y preparamos un puerto localhost libre. Preparar un documento 'docker-compose.yaml' version: \"3.9\" services: redis: container_name: redis_om_python_demo image: \"redislabs/redisearch:edge\" ports: - 6379:6379 deploy: replicas: 1 restart_policy: condition: on-failure Instalamos los paquetes necesarios (algunos de ellos est\u00e1n en requirements.txt ) pip install -r requirements.txt En otra terminal ejecutamos docker-compose up y habremos a\u00f1adido el container que quer\u00edamos en docker. 3) REDIS CLOUD set REDIS_OM_URL = redis://username:password@public-endpoint/dataset_name Blog explicado RedisInsight Link: RedisInsight Es una interfaz para ver qu\u00e9 est\u00e1 pasando. Traer la base de datos especificando host y port . Si estamos en cloud tambi\u00e9n tendremos que meter la contrase\u00f1a. Creamos un Redis Model en OM: from typing import Text from redis_om import (Field, HashModel) from redisearch.client import TagField class Employee(HashModel): firstName: str = Field(index=True) lastName: str = Field(index=True) salary: int = Field(index=True) department: str = Field(index=True) isAdmin: int = Field(index=True) Lo que estamos modelando se guardar\u00e1 como un hash en redis. Le decimos los campos (Field) que queremos que guarde y que queremos que los indexe. Lo indexar\u00e1 con redisearch y crear\u00e1 un \u00edndice para mantenimiento y b\u00fasqueda. Cada persona se guardar\u00e1 asociado a un valor (tal y como funciona redis, con clave-valor), pero nosostros queremos ser capaces de hacer b\u00fasquedas dependiendo de alg\u00fan campo. Reisearch nos ayudar\u00e1 en esto. Si quisieramos modelar algo con campos anidados (que no sea clave-valor como en este caso) podr\u00edamos utilizar el modelado JSON de redis. Ambos son modelos pedantic (una librer\u00eda de validaci\u00f3n de python) por lo que podemos a\u00f1adir criterios adicionales (que un campo sea un email, que un int est\u00e9 entre valores que especifiquemos...). Cargar datos en redis Creamos una base de datos en redis a partir de un CSV. Es algo que ejecutaremos solamente una vez. import csv from employee import Employee from redis_om import Migrator with open('employee.csv') as csv_file: employees = csv.DictReader(csv_file) for employee in employees: emp = Employee(**employee) # print(f\"{employee['firstName']} -> {emp.pk}\") emp.save() # Create a RediSearch index Migrator().run() Con pk estamos creando claves que identifiquen a cada persona como \u00fanico localmente, antes de guardarlo en redis. Con Migrator().run() activamos algo parecido a \"un detector de cambios\". As\u00ed todo lo que hagamos estar\u00e1 registrado siempre. Lo ejecutamos en la terminal: py upload_employee.py Podemos verificar que de verdad se ha subido en RedisInsight Si queremos conectarnos con Redis Cloud Ejecutamos en el cmd: set REDIS_OM_URL = redis://username:password@public-endpoint/dataset_name Y realizamos los pasos de arriba para cargar los datos. Me ha dado error y lo hemos forzado en el c\u00f3digo. Definimos la conexi\u00f3n a nuestra base de datos antes de importar lo dem\u00e1s. import csv import os os.environ[\"REDIS_OM_URL\"]=\"redis://default:CgdemOw59KKOuX4ZugQVaThJv738hBAf@redis-10742.c124.us-central1-1.gce.cloud.redislabs.com:10742/books\" from employee import Employee from redis_om import Migrator with open('employe.csv') as csv_file: employees = csv.DictReader(csv_file) for employee in employees: emp = Employee(**employee) print(f\"{employee['firstName']} -> {emp.pk}\") emp.save() # Create a RediSearch index Migrator().run() Queries Podemos hacerlo con redisOM o redisearch. Redisearch Para Redisearch necesitamos definir un client . from redisearch import Client, IndexDefinition, TextField, NumericFieldTagField, GeoField def initializeClient(): SCHEMA = ( TextField(\"firstName\"), TextField(\"lastName\"), NumericField(\"salary\"), TextField(\"department\"), NumericField(\"isAdmin\"), TagField(\"tag\"), GeoField(\"location\") ) client = Client(\"myIndex\") definition = IndexDefinition(prefix=[':employee.Employee:']) # lo que tienen en com\u00fan antes del hash try: # No podemos cargar un cliente con el mismo index client.info() except ResponseError: # Index does not exist. We need to create it! client.create_index(SCHEMA, definition=definition) return client client = initializeClient() EJEMPLOS Imports from employee import Employee from redis import ResponseError from redisearch import Client, IndexDefinition, TextField, NumericField, TagField, GeoField from redisearch import reducers from redisearch.aggregation import AggregateRequest, Asc, Desc Task 1 : find by first name OM: def find_by_first_name(): return Employee.find(Employee.firstName == 'ahmad').all() # search based on first name (could be done like this) Redisearch: def find_by_first_name_redisearch(client): res = client.search(\"@firstName:ahmad\") return res Task 2 : find by first name (autocompletando con redisearch) def find_by_name_wildcard_redisearch(client): res = client.search(\"@firstName:br*\") for result in res.docs: print(result) Task 3: Find by first and last name con OM def find_by_first_and_last_name(): return Employee.find((Employee.firstName == 'ahmad') & (Employee.lastName == 'bazzi')).all() Task 4: Sort in ascending OM: def sort_by_salary(): return Employee.find(Employee.salary>0).sort_by(\"salary\") Task 5: Sort in descending with redisearch: def sort_by_salary_redisearch_descending(): request = AggregateRequest('*').group_by(['@salary','@firstName'], reducers.count().alias('count')).sort_by(Desc('@salary')) result = client.aggregate(request) for r in result.rows: print(r)","title":"REDIS"},{"location":"redis/#redis","text":"redis-py https://try.redis.io/","title":"REDIS"},{"location":"redis/#basic-data-structures","text":"","title":"BASIC DATA STRUCTURES"},{"location":"redis/#keys-and-expiration","text":"Documentation Keys are the primary way to access data values. Hay 16 databases, por defecto siempre se utilizar\u00e1 database0. EJEMPLO (por convenci\u00f3n, pero se puede hacer de otra forma): \"user:1000:followers\" user: nombre del objeto 1000: identificador \u00fanico de la instancia followers: objeto compuesto","title":"Keys and Expiration"},{"location":"redis/#set","text":"Para asignar un valor a una llave. SET customer:1000 \"fred\" En nuestra llave (cliente de ID 1000) customer:1000 estar\u00e1 guardado el valor \"fred\" Si hacemos SET a una llave que no existe la crearemos. si no queremos que esto suceda podemos utilizar NX SET provider:100 \"freddy\" NX Nos retornar\u00e1 OK si no existe y nos devolver\u00e1 NIL si existe.","title":"SET"},{"location":"redis/#get","text":"Para obtener el valor que tiene guardada la llave customer:1000 GET customer:1000 => fred En cambio, si queremos que la llave exista antes de hacer SET utilizaremos XX de la misma manera.","title":"GET"},{"location":"redis/#keys-and-scan","text":"KEYS itera sobre todas las llaves, para ver si existe la que le hemos pedido. Si la base de datos es grande podr\u00eda tardar demasiado. SCAN itera usando un cursor y devuelve un espacio de referencia. Est\u00e1 bien para usarlo en producci\u00f3n. EJEMPLOS: KEYS Todos los clientes cuyo ID empiezan con 1: keys customer:1* => customer:1500 customer:1000 SCAN SCAN slot [MATCH pattern][COUNT count] scan 0 MATCH customer: 1* => 1) 14336 2) empty list or set scan 14336 MATCH customer:1* => 1) 14848 2) empty list or set scan 14848 MATCH customer:1* COUNT 10000 => 1) 1229 2) 1)customer:1500 2)customer:1000 scan 1229 MATCH customer:1* => 1) 0 2) empty list or set Cuando aparece 0 significa que no hay nada m\u00e1s sobre lo que iterar. ...","title":"KEYS and SCAN"},{"location":"redis/#borrar-llaves","text":"","title":"Borrar llaves"},{"location":"redis/#del","text":"Para borrar una llave y el valor asociado a ella o desvincular su llave con el valor. DEL key[key...] UNLINK key[key]","title":"DEL"},{"location":"redis/#exists","text":"EXISTS server:name => 1 (existe) EXISTS server:blabla => 0 (no existe) Es una base de datos que funciona por llave-valor. Un valor siempre se guardar\u00e1 dentro de una llave y la \u00fanica forma de obtener dicho valor es tener la llave. Se explicar\u00e1n los comandos m\u00e1s b\u00e1sicos a continuaci\u00f3n, pero la lista completa esta aqu\u00ed","title":"EXISTS"},{"location":"redis/#incr-y-incrby","text":"Incrementan el valor de una llave dada. INCR lo incrementa por 1 y INCRBY lo incrementa por el valor que nosotros le proporcionemos. Son operaciones at\u00f3micas. SET connections 10 INCR connections GET connections => 11 SET connections 10 INCRBY connections 100 GET connections => 110","title":"INCR y INCRBY"},{"location":"redis/#decr-y-decrby","text":"Decrementan el valor de una llave dada. INCR lo incrementa por 1 y INCRBY lo incrementa por el valor que nosotros le proporcionemos.","title":"DECR y DECRBY"},{"location":"redis/#expire-y-ttl","text":"Para decir que una llave tiene que existir solo para un tiempo especificado en segundos. SET resource:lock \"Redis Demo\" EXPIRE resource:lock 120 Con TTL podremos mirar cuento le queda a una llave: TTL resource:lock => 113 Si TTL es -1 significa que esa llave es para siempre.","title":"EXPIRE y TTL"},{"location":"redis/#listas","text":"","title":"LISTAS"},{"location":"redis/#python","text":"redis-py library pip install redis","title":"PYTHON"},{"location":"redis/#connect-to-redis","text":"import os import pytest import redis USERNAME = os.environ.get('REDISOLAR_REDIS_USERNAME') PASSWORD = os.environ.get('REDISOLAR_REDIS_PASSWORD') @pytest.fixture def redis_connection(app): client_kwargs = { \"host\": app.config['REDIS_HOST'], \"port\": app.config['REDIS_PORT'], \"decode_responses\": True } if USERNAME: client_kwargs[\"username\"] = USERNAME if PASSWORD: client_kwargs[\"password\"] = PASSWORD yield redis.Redis(**client_kwargs) def test_say_hello(redis_connection): result = redis_connection.set(\"hello\", \"world\") value = redis_connection.get(\"hello\") assert result is True assert value == \"world\"","title":"Connect to redis:"},{"location":"redis/#redis-clients","text":"Gestiona conexiones, implementa el protocolo redis (RESP) y nos deja usar un lenguaje sencillo (GET, SET, INCR...). Walrus redis-py Utilizaremos redis-py como nuestra librer\u00eda cliente.","title":"Redis Clients"},{"location":"redis/#como-conectarse-a-redis-utilizando-redis-py","text":"import redis from redis.sentinel import Sentinel from rediscluster import RedisCluster def connection_examples(): # Connect to a standard Redis deployment. client = redis.Redis(\"localhost\", port=6379, decode_responses=True, max_connections=20) # Read and write through the client. client.set(\"foo\", \"bar\") client.get(\"foo\")","title":"C\u00f3mo conectarse a redis utilizando redis-py"},{"location":"redis/#basic-operations","text":"Redys type > Python type: String > bytes or string List > list of bytes or strings Set > set of bytes or strings Hash > dictionary PLANETS = [ \"Mercury\", \"Mercury\", \"Venus\", \"Earth\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\" ] EARTH_KEY = \"earth\" ## LISTS def test_redis_list(redis, key_schema): key = key_schema.planets_list_key() assert len(PLANETS) == 11 # Add all test planets to a Redis list result = redis.rpush(key, *PLANETS) # Check that the length of the list in Redis is the same assert result == len(PLANETS) # Get the planets from the list # Note: LRANGE is an O(n) command. Be careful running this command # with large lists. planets = redis.lrange(key, 0, -1) assert planets == PLANETS # Remove the elements that we know are duplicates # Note: O(n) operation. redis.lrem(key, 1, \"Mercury\") redis.lrem(key, 1, \"Earth\") planet = redis.rpop(key) assert planet == \"Pluto\" assert redis.llen(key) == 8 ## SETS def test_redis_set(redis, key_schema): key = key_schema.planets_set_key() # Add planets to a Redis set redis.sadd(key, *PLANETS) # Return the cardinality of the set assert redis.scard(key) == 9 # Fetch all values from the set # Note: SMEMBERS is an O(n) command. Be careful running this command # with high-cardinality sets. Consider SSCAN as an alternative. assert redis.smembers(key) == set(PLANETS) # Pluto is, of course, no longer a first-class planet. Remove it. response = redis.srem(key, \"Pluto\") assert response == 1 # Now we have 8 planets, as expected. assert redis.scard(key) == 8 ## HASHES def test_redis_hash(redis): earth_properties = { \"diameter_km\": \"12756\", \"day_length_hrs\": \"24\", \"mean_temp_c\": \"15\", \"moon_count\": \"1\" } # Set the fields of the hash. redis.hset(EARTH_KEY, mapping=earth_properties) # Get the hash we just created back from Redis. stored_properties = redis.hgetall(EARTH_KEY) assert stored_properties == earth_properties # Test that we can get a single property. assert redis.hget(EARTH_KEY, \"diameter_km\") == earth_properties[\"diameter_km\"] CUIDADO!! Algunos m\u00e9todos pueden ser lentos. Para muchos datos, hay que utilizar los m\u00e9todos de la familia SCAN.","title":"Basic operations"},{"location":"redis/#daos","text":"Get started with redis cloud: Create a subscription and a database. To connect with the database we need: public endpoint for the host and port password for the authorization First code: import redis redis = redis.Redis( host='redis-16018.c228.us-central1-1.gce.cloud.redislabs.com', port='16018', password='AAa6DTbT7c0kYoZuVDM3mLTrEuGMinDt') redis.set('mykey', 'Hello from Python!') value = redis.get('mykey') print(value) redis.zadd('vehicles', {'car': 0}) redis.zadd('vehicles', {'bike': 0}) vehicles = redis.zrange('vehicles', 0, -1) print(vehicles) Output: b'Hello from Python!' [b'bike', b'car']","title":"DAOs"},{"location":"redis/#redisearch","text":"Tutorial Aggregations tutorial RediSearch en python Redisearch es una herramienta de b\u00fasqueda. pip install redisearch Crear una instancia cliente. tutorial from redisearch import Client client = Client(\"my-index\") Client Python API from redis import ResponseError from redisearch import Client, IndexDefinition, TextField SCHEMA = ( TextField(\"title\", weight=5.0), TextField(\"body\") ) client = Client(\"my-index\") definition = IndexDefinition(prefix=['blog:']) try: client.info() except ResponseError: # Index does not exist. We need to create it! client.create_index(SCHEMA, definition=definition) from redisearch import Client, TextField, NumericField, Query # Creating a client with a given index name client = Client('myIndex') # Creating the index definition and schema client.create_index([TextField('title', weight=5.0), TextField('body')]) # Indexing a document client.add_document('doc1', title = 'RediSearch', body = 'Redisearch implements a search engine on top of redis') # Simple search res = client.search(\"search engine\") # the result has the total number of results, and a list of documents print res.total # \"1\" print res.docs[0].title # Searching with snippets res = client.search(\"search engine\", snippet_sizes = {'body': 50}) # Searching with complex parameters: q = Query(\"search engine\").verbatim().no_content().paging(0,5) res = client.search(q)","title":"REDISEARCH"},{"location":"redis/#querying","text":"","title":"QUERYING"},{"location":"redis/#finding-exact-string-matches","text":"","title":"Finding exact string matches"},{"location":"redis/#redis-om","text":"GitHub Yotube tutorial Youtube tutorial Curso de redis EJEMPLO: Vamos a montar un refugio de animales y tenemos un csv con los siguientes campos: name, species, age, weight, sex, fee, children, other_animals, description .","title":"REDIS OM"},{"location":"redis/#creamos-un-redis-model-en-om","text":"from typing import Text from redis_om import (Field, HashModel) class Adoptable (HashModel): name: str = Field(index=True) species: str = Field(index=True) age: str = Field(index=True) weight: float = Field(index=True) sex: str = Field(index=True) fee: str = Field(index=True) children: str = Field(index=True) other_animals: str = Field(index=True) description: str = Field(index=True, full_text_search=True) #Le estamos diciendo a redis c\u00f3mo queremos que lo indexe, ya que queremos ser capaces de hacer un 'full text search' (utilizar\u00e1 un query de redisearch) Lo que estamos modelando se guardar\u00e1 como un hash en redis. Le decimos los campos (Field) que queremos que guarde y que queremos que los indexe. Lo indexar\u00e1 con redisearch y crear\u00e1 un \u00edndice para mantenimiento y b\u00fasqueda. De esta forma, cuando creemos instancias, client gestionar\u00e1 esto volviendolo a guardar en redis. Cada animal se guardar\u00e1 asociado a un valor (tal y como funciona redis, con clave-valor), pero nosostros queremos ser capaces de hacer b\u00fasquedas dependiendo de alg\u00fan campo (ej. queremos un perro que pese menos de 20kg). Reisearch nos ayudar\u00e1 en esto. Si quisieramos modelar algo con campos anidados (que no sea clave-valor como en este caso) podr\u00edamos utilizar el modelado JSON de redis. Ambos son modelos pedantic (una librer\u00eda de validaci\u00f3n de python) por lo que podemos a\u00f1adir criterios adicionales (que un campo sea un email, que un int est\u00e9 entre valores que especifiquemos...).","title":"Creamos un Redis Model en OM:"},{"location":"redis/#cargar-datos-en-redis","text":"Creamos una base de datos en redis a partir de un CSV: import csv from adoptable import Adoptable from redis_om impor Migrator with open['animal_data.csv'] as csv_file: animail_reader = csv.DictReader(csv_file) for animal in animal_reader: adoptable = Adoptable(**animal) print(f\"{animal[name]} has pk = {adoptable.pk}\") # pk lo crea redis OM como clave primaria adoptable.save() # Hacer la migraci\u00f3n de datos Migrator().run() Con pk estamos creando claves que identifiquen a cada animal como \u00fanico localmente antes de guardarlo en redis. Con Migrator.run() activamos algo parecido a \"un detector de cambios\". As\u00ed todo lo que hagamos estar\u00e1 registrado siempre. Podemos descargarnos RedisInsight para interactuar con nuestra base de datos. Es una interfaz donde podemos ver y gestionar nuestros datos f\u00e1cilmente.","title":"Cargar datos en redis"},{"location":"redis/#querys","text":"from adoptable import Adoptable def show_result(results): for adoptable in results: print(adoptable) print(\"\") def find_by_name(): print(\"find by name: \") return Adoptable.find(Adoptable.name == \"Luna\").all() def find_male_dogs(): print(\"find male dogs: \") return Adoptable.find((Adoptable.species == \"dog\") & (Adoptable.sex == \"m\") ) def find_dogs_in_age_range(): print(\"find dogs in ages range: \") return Adoptable.find((Adoptable.species == \"dog\") & (Adoptable.age < 8) & (Adoptable.age < 11) ).sort_by(\"age\") def find_cats_good_with_children(): print(\"find cats good with children: \") return Adoptable.find((Adoptable.species == \"cat\") & (Adoptable.children == \"y\") & (Adoptable.description % \"play\") & ~(Adoptable.description % \"anxious\") & ~(Adoptable.description % \"nervous\") ) show_results(find_by_name()) En la \u00faltima funci\u00f3n nos basamos en redisearch para las b\u00fasquedas. Si queremos buscar m\u00e1s all\u00e1 de valores concretos, nos podemos fijar en la descripci\u00f3n. Al buscar con % lo que hacemos es buscar una palabra parecida a la que estamos buscando, incluyendo tambi\u00e9n esa palabra. En cambio, con el s\u00edmbolo ~ lo que hacemos es excluir esa palabra y parecidas de nuestra b\u00fasqueda.","title":"Querys"},{"location":"redis/#redis_om-y-redisearch","text":"Tutorial redisearch Aggregations tutorial redisearch RediSearch en python GitHub Redis_OM Yotube tutorial Youtube tutorial Curso de redis oficial Vamos a manejar una base de datos con los siguientes campos: firstName,lastName,salary,department,isAdmin","title":"REDIS_OM y REDISEARCH"},{"location":"redis/#iniciar-redis-en-windows","text":"Tres opciones: 1) WSL DE LINUX 2) CON DOCKER Instalar docker y preparamos un puerto localhost libre. Preparar un documento 'docker-compose.yaml' version: \"3.9\" services: redis: container_name: redis_om_python_demo image: \"redislabs/redisearch:edge\" ports: - 6379:6379 deploy: replicas: 1 restart_policy: condition: on-failure Instalamos los paquetes necesarios (algunos de ellos est\u00e1n en requirements.txt ) pip install -r requirements.txt En otra terminal ejecutamos docker-compose up y habremos a\u00f1adido el container que quer\u00edamos en docker. 3) REDIS CLOUD set REDIS_OM_URL = redis://username:password@public-endpoint/dataset_name Blog explicado","title":"Iniciar Redis en Windows"},{"location":"redis/#redisinsight","text":"Link: RedisInsight Es una interfaz para ver qu\u00e9 est\u00e1 pasando. Traer la base de datos especificando host y port . Si estamos en cloud tambi\u00e9n tendremos que meter la contrase\u00f1a.","title":"RedisInsight"},{"location":"redis/#creamos-un-redis-model-en-om_1","text":"from typing import Text from redis_om import (Field, HashModel) from redisearch.client import TagField class Employee(HashModel): firstName: str = Field(index=True) lastName: str = Field(index=True) salary: int = Field(index=True) department: str = Field(index=True) isAdmin: int = Field(index=True) Lo que estamos modelando se guardar\u00e1 como un hash en redis. Le decimos los campos (Field) que queremos que guarde y que queremos que los indexe. Lo indexar\u00e1 con redisearch y crear\u00e1 un \u00edndice para mantenimiento y b\u00fasqueda. Cada persona se guardar\u00e1 asociado a un valor (tal y como funciona redis, con clave-valor), pero nosostros queremos ser capaces de hacer b\u00fasquedas dependiendo de alg\u00fan campo. Reisearch nos ayudar\u00e1 en esto. Si quisieramos modelar algo con campos anidados (que no sea clave-valor como en este caso) podr\u00edamos utilizar el modelado JSON de redis. Ambos son modelos pedantic (una librer\u00eda de validaci\u00f3n de python) por lo que podemos a\u00f1adir criterios adicionales (que un campo sea un email, que un int est\u00e9 entre valores que especifiquemos...).","title":"Creamos un Redis Model en OM:"},{"location":"redis/#cargar-datos-en-redis_1","text":"Creamos una base de datos en redis a partir de un CSV. Es algo que ejecutaremos solamente una vez. import csv from employee import Employee from redis_om import Migrator with open('employee.csv') as csv_file: employees = csv.DictReader(csv_file) for employee in employees: emp = Employee(**employee) # print(f\"{employee['firstName']} -> {emp.pk}\") emp.save() # Create a RediSearch index Migrator().run() Con pk estamos creando claves que identifiquen a cada persona como \u00fanico localmente, antes de guardarlo en redis. Con Migrator().run() activamos algo parecido a \"un detector de cambios\". As\u00ed todo lo que hagamos estar\u00e1 registrado siempre. Lo ejecutamos en la terminal: py upload_employee.py Podemos verificar que de verdad se ha subido en RedisInsight","title":"Cargar datos en redis"},{"location":"redis/#si-queremos-conectarnos-con-redis-cloud","text":"Ejecutamos en el cmd: set REDIS_OM_URL = redis://username:password@public-endpoint/dataset_name Y realizamos los pasos de arriba para cargar los datos. Me ha dado error y lo hemos forzado en el c\u00f3digo. Definimos la conexi\u00f3n a nuestra base de datos antes de importar lo dem\u00e1s. import csv import os os.environ[\"REDIS_OM_URL\"]=\"redis://default:CgdemOw59KKOuX4ZugQVaThJv738hBAf@redis-10742.c124.us-central1-1.gce.cloud.redislabs.com:10742/books\" from employee import Employee from redis_om import Migrator with open('employe.csv') as csv_file: employees = csv.DictReader(csv_file) for employee in employees: emp = Employee(**employee) print(f\"{employee['firstName']} -> {emp.pk}\") emp.save() # Create a RediSearch index Migrator().run()","title":"Si queremos conectarnos con Redis Cloud"},{"location":"redis/#queries","text":"Podemos hacerlo con redisOM o redisearch.","title":"Queries"},{"location":"redis/#redisearch_1","text":"Para Redisearch necesitamos definir un client . from redisearch import Client, IndexDefinition, TextField, NumericFieldTagField, GeoField def initializeClient(): SCHEMA = ( TextField(\"firstName\"), TextField(\"lastName\"), NumericField(\"salary\"), TextField(\"department\"), NumericField(\"isAdmin\"), TagField(\"tag\"), GeoField(\"location\") ) client = Client(\"myIndex\") definition = IndexDefinition(prefix=[':employee.Employee:']) # lo que tienen en com\u00fan antes del hash try: # No podemos cargar un cliente con el mismo index client.info() except ResponseError: # Index does not exist. We need to create it! client.create_index(SCHEMA, definition=definition) return client client = initializeClient()","title":"Redisearch"},{"location":"redis/#ejemplos","text":"","title":"EJEMPLOS"},{"location":"redis/#imports","text":"from employee import Employee from redis import ResponseError from redisearch import Client, IndexDefinition, TextField, NumericField, TagField, GeoField from redisearch import reducers from redisearch.aggregation import AggregateRequest, Asc, Desc","title":"Imports"},{"location":"redis/#task-1-find-by-first-name","text":"OM: def find_by_first_name(): return Employee.find(Employee.firstName == 'ahmad').all() # search based on first name (could be done like this) Redisearch: def find_by_first_name_redisearch(client): res = client.search(\"@firstName:ahmad\") return res","title":"Task 1 : find by first name"},{"location":"redis/#task-2-find-by-first-name-autocompletando-con-redisearch","text":"def find_by_name_wildcard_redisearch(client): res = client.search(\"@firstName:br*\") for result in res.docs: print(result)","title":"Task 2 : find by first name (autocompletando con redisearch)"},{"location":"redis/#task-3-find-by-first-and-last-name-con-om","text":"def find_by_first_and_last_name(): return Employee.find((Employee.firstName == 'ahmad') & (Employee.lastName == 'bazzi')).all()","title":"Task 3: Find by first and last name con OM"},{"location":"redis/#task-4-sort-in-ascending","text":"OM: def sort_by_salary(): return Employee.find(Employee.salary>0).sort_by(\"salary\")","title":"Task 4: Sort in ascending"},{"location":"redis/#task-5-sort-in-descending-with-redisearch","text":"def sort_by_salary_redisearch_descending(): request = AggregateRequest('*').group_by(['@salary','@firstName'], reducers.count().alias('count')).sort_by(Desc('@salary')) result = client.aggregate(request) for r in result.rows: print(r)","title":"Task 5: Sort in descending with redisearch:"},{"location":"simpy/","text":"SIMPY EJEMPLO 1 import simpy def alarm(env): yield env.timeout(10) print(\"time to wake up!) Ese es un proceso. env es el enviroment de simpy. Controla en qu\u00e9 punto del tiempo est\u00e1 la simulacion. Los procesos pertenecen a un env . yield puede ser le\u00eddo en este contexto como \"wait for\", (es parecido a un await ). Este c\u00f3digo, solamente imprimir\u00e1 \"time to wake up!\". Los 10 que decimos que le espere son abstractos. Un dato que se guardar\u00e1 en el enviroment , pero no se relacionan con tiempo real. EJEMPLO 2 import simpy def alarm(env): yield env.timeout(10) print(\"time to wake up!\") def alarm_2(env): yield env.timeout(200) print(\"wake up again!\") env = simpy.Environment() env.process(alarm(env)) env.process(alarm_2(env)) env.run() Da igual en que orden llamemos a la alarma 1 o 2. Siempre se imprimir\u00e1 primero la de la 1 y luego la de la 2. \u00c9l va contando por dentro y sabe que la alarma uno tiene que ejecutarse a las 10 unidades y la 2 a las 200 unidades. EJEMPLO DE UN BAR import simpy def barista(env): TIME_TO_MAKE_COFEE = 5 while True: yield wait_for_customer() yield env.tiemout(TIME_TO_MAKE_COFFE) El barista espera indefinidamente para un cliente, cuando le llega hace el caf\u00e9 en 5 unidades de tiempo. RESOURCES get() Coger de un recurso put() A\u00f1adir a un recurso Tipos de recurso Sem\u00e1foros Suministro homog\u00e9neo Bolsa de objetos import simpy def barista(env): TIME_TO_MAKE_COFEE = 5 while True: # Request 1 customer (to make the coffee for) yield customer_resource.get(1) # Make the coffee yield env.tiemout(TIME_TO_MAKE_COFFE) def arrivals(env, customer_resource, arrival_times): for arrival_time in arrival_times: # Wait until the next arrival time yield env.timeout(arrival_time - env.now) # Add a customer yield customer_resource.put(1) env = simpy.Enviroment() # I've created a LogginWrapper to log when the resource level changes customer_resource = LogginWrapper(simpy.container(env)) env.process(arrivals(env, cursomer_resource, arrival_times)) env.process(arista(env, customer_resource)) env.run(until=8 * 68)","title":"SIMPY"},{"location":"simpy/#simpy","text":"","title":"SIMPY"},{"location":"simpy/#ejemplo-1","text":"import simpy def alarm(env): yield env.timeout(10) print(\"time to wake up!) Ese es un proceso. env es el enviroment de simpy. Controla en qu\u00e9 punto del tiempo est\u00e1 la simulacion. Los procesos pertenecen a un env . yield puede ser le\u00eddo en este contexto como \"wait for\", (es parecido a un await ). Este c\u00f3digo, solamente imprimir\u00e1 \"time to wake up!\". Los 10 que decimos que le espere son abstractos. Un dato que se guardar\u00e1 en el enviroment , pero no se relacionan con tiempo real.","title":"EJEMPLO 1"},{"location":"simpy/#ejemplo-2","text":"import simpy def alarm(env): yield env.timeout(10) print(\"time to wake up!\") def alarm_2(env): yield env.timeout(200) print(\"wake up again!\") env = simpy.Environment() env.process(alarm(env)) env.process(alarm_2(env)) env.run() Da igual en que orden llamemos a la alarma 1 o 2. Siempre se imprimir\u00e1 primero la de la 1 y luego la de la 2. \u00c9l va contando por dentro y sabe que la alarma uno tiene que ejecutarse a las 10 unidades y la 2 a las 200 unidades.","title":"EJEMPLO 2"},{"location":"simpy/#ejemplo-de-un-bar","text":"import simpy def barista(env): TIME_TO_MAKE_COFEE = 5 while True: yield wait_for_customer() yield env.tiemout(TIME_TO_MAKE_COFFE) El barista espera indefinidamente para un cliente, cuando le llega hace el caf\u00e9 en 5 unidades de tiempo.","title":"EJEMPLO DE UN BAR"},{"location":"simpy/#resources","text":"","title":"RESOURCES"},{"location":"simpy/#get","text":"Coger de un recurso","title":"get()"},{"location":"simpy/#put","text":"A\u00f1adir a un recurso","title":"put()"},{"location":"simpy/#tipos-de-recurso","text":"Sem\u00e1foros Suministro homog\u00e9neo Bolsa de objetos","title":"Tipos de recurso"},{"location":"simpy/#_1","text":"import simpy def barista(env): TIME_TO_MAKE_COFEE = 5 while True: # Request 1 customer (to make the coffee for) yield customer_resource.get(1) # Make the coffee yield env.tiemout(TIME_TO_MAKE_COFFE) def arrivals(env, customer_resource, arrival_times): for arrival_time in arrival_times: # Wait until the next arrival time yield env.timeout(arrival_time - env.now) # Add a customer yield customer_resource.put(1) env = simpy.Enviroment() # I've created a LogginWrapper to log when the resource level changes customer_resource = LogginWrapper(simpy.container(env)) env.process(arrivals(env, cursomer_resource, arrival_times)) env.process(arista(env, customer_resource)) env.run(until=8 * 68)","title":""},{"location":"sql/","text":"SQL Lenguaje para base de datos relacionales. Bases de datos relacionales Clave primaria: la que aparece solo una vez. Es el identificador. Campo: Columnas Registro: Filas Las tablas se pueden unir entre ellas utilizando campos que tienen en com\u00fan. Se hace un esquema. Clave foranea: Claves que son primarias en otras tablas, y que sirven para conectarse con ellas. Consultas de SQL B\u00e1sico SELECT campo1, campo2 FROM tabla WHERE condicion; /*El punto y coma no es necesari*/ ORDENADO: SELECT * FROM tabla WHERE campo ASC/DESC; Tipos de dato N\u00fameros Cadenas de caracteres Fechas Cada tipo de dato tiene una forma de manipulaci\u00f3n. Cadenas de caracteres Buscar cadenas de caracteres que contengan una palabra espec\u00edfica SELECT campo FROM tabla WHERE descripcion LIKE '%palabra_especifica%' CAMBIAR UN CAMPO SELECT campo1, campo2, campo_nombre_viejo AS campo_nombre_nuevo FROM tabla CONCATENAR VALORES DE CAMPO SELCT nombre, apellido CONCAT(nombre, ' ', apellido) AS nombre_y_apellido FROM tabla /* Se seleccionar\u00e1n tres columnas: nombre, apellido y nombre_apellido Aparte de CONCAT para cadenas de car\u00e1cteres: UPPER() LOWER() LENGHT() Fechas NOW() YEAR( NOW() ) DATE() MONTH() DAY() N\u00fameros MIN() Elige el m\u00ednimo de un campo MAX() Elige un m\u00e1ximo de un campo COUNT() Cuenta la cantidad de registros de un campo SUM() Sumar\u00e1 los valores de un campo DISTINCT SELECT DISTINCT campo FROM tabla /*Nos seleccionar\u00e1 los registros diferentes de un campo*/ SELECT COUNT (DISTINCT campo) FROM tabla /*Nos dir\u00e1 cu\u00e1ntos registros deferentes tenemos en un campo*/ GROUP BY SELCT costo_renta, COUNT(*) as cantidad FROM pelicula GORUP BY costo_renta --------------------------- | costo_renta | cantidad | --------------------------- | 0.99 | 341 | | 2.99 | 323 | | 4.99 | 336 | --------------------------- Nos dir\u00e1 que tenemos 341 pel\u00edculas de costo 0.99, 323 pel\u00edculas de costo 2.99 y 336 pel\u00edculas de costo 4.99. OTRO EJEMPLO: SELECT tienda_id, COUNT(DISTINCT pelicula_id) AS peliculas, COUNT(inventario_id) AS copias FROM inventario GROUP BY tienda_id Nos dir\u00e1 cuantas pel\u00edculas hay por cada tienda y cuantas copias diferentes tambi\u00e9n de cada tienda. Tambi\u00e9n podemos agrupar por m\u00e1s de un campo. Nos agrupar\u00e1 todos los registros que tengan los dos campos en com\u00fan. AVG AVG(campo) /*Nos dir\u00e1 el promedio de un campo*/ JOIN Para cruzar tablas. (Devuelve registro que tienen registros en ambas tablas) SELECT tabla1.campo1, tabla1.campo2, tabla2.campo1 FROM tabla1 JOIN tabla2 ON tabla2.campo_igual = tabla1.campo_igual Podemos cambiar la notaci\u00f3n: SELECT A.campo1, A.campo2, B.campo1 FROM tabla1 A JOIN tabla2 B ON B.campo_igual = A.campo_igual Tambi\u00e9n podemos hacer JOIN con m\u00e1s de dos tablas. Escribiremos dos JOIN para ello. LEFT JOIN y RIGHT JOIN Si queremos todos los registros, aunque no est\u00e9n en ambas tablas. Left nos devolver\u00e1 todo lo que est\u00e1 en tabla1 aunque no est\u00e9 en tabla2. Right har\u00e1 lo contrario. EJEMPLO: Nos dir\u00e1 cu\u00e1ntas copias de cada pelicula (existente en la tienda1) hay en la tienda1: SELECT p.pelicula_id p.titulo COUNT(i.inventario_id) AS copias FROM pelicula p LEFT JOIN inventario i ON p.pelicula_id=i.pelicula_id WHERE i.tienda_id=1 GROUP BY p.pelicula_id, p.titulo --------------------------------------- | pelicula_id | cantidad | copias | --------------------------------------- | 1 | titulo1 | 8 | | 3 | titulo2 | 3 | | 6 | titulo3 | 4 | | ... | ... | ... | --------------------------------------- Sin utilizar WHERE : Nos dir\u00e1 cu\u00e1ntas copias de cada pelicula (existente en la tienda1) hay en la tienda1: SELECT p.pelicula_id p.titulo COUNT(i.inventario_id) AS copias FROM pelicula p LEFT JOIN inventario i ON p.pelicula_id=i.pelicula_id AND i.tienda_id=1 GROUP BY p.pelicula_id, p.titulo --------------------------------------- | pelicula_id | cantidad | copias | --------------------------------------- | 1 | titulo1 | 8 | | 2 | titulo2 | 0 | | 3 | titulo3 | 3 | | ... | ... | ... | --------------------------------------- CUIDADO AL UTILIZAR WHERE !! CONDICIONALES SELECT campo1, campo2 CASE WHEN campo2<3 THEN 'peque\u00f1o' WHEN campo2>3 AND campo2<5 THEN 'MEDIANO' / (ELIF)Esta no se va a evaluar si la anterior ha sido correcta / ELSE 'grande' END AS tama\u00f1o FROM tabla BIG QUERY Documentaci\u00f3n","title":"SQL"},{"location":"sql/#sql","text":"Lenguaje para base de datos relacionales.","title":"SQL"},{"location":"sql/#bases-de-datos-relacionales","text":"Clave primaria: la que aparece solo una vez. Es el identificador. Campo: Columnas Registro: Filas Las tablas se pueden unir entre ellas utilizando campos que tienen en com\u00fan. Se hace un esquema. Clave foranea: Claves que son primarias en otras tablas, y que sirven para conectarse con ellas.","title":"Bases de datos relacionales"},{"location":"sql/#consultas-de-sql","text":"","title":"Consultas de SQL"},{"location":"sql/#basico","text":"SELECT campo1, campo2 FROM tabla WHERE condicion; /*El punto y coma no es necesari*/ ORDENADO: SELECT * FROM tabla WHERE campo ASC/DESC;","title":"B\u00e1sico"},{"location":"sql/#tipos-de-dato","text":"N\u00fameros Cadenas de caracteres Fechas Cada tipo de dato tiene una forma de manipulaci\u00f3n.","title":"Tipos de dato"},{"location":"sql/#cadenas-de-caracteres","text":"Buscar cadenas de caracteres que contengan una palabra espec\u00edfica SELECT campo FROM tabla WHERE descripcion LIKE '%palabra_especifica%' CAMBIAR UN CAMPO SELECT campo1, campo2, campo_nombre_viejo AS campo_nombre_nuevo FROM tabla CONCATENAR VALORES DE CAMPO SELCT nombre, apellido CONCAT(nombre, ' ', apellido) AS nombre_y_apellido FROM tabla /* Se seleccionar\u00e1n tres columnas: nombre, apellido y nombre_apellido Aparte de CONCAT para cadenas de car\u00e1cteres: UPPER() LOWER() LENGHT()","title":"Cadenas de caracteres"},{"location":"sql/#fechas","text":"NOW() YEAR( NOW() ) DATE() MONTH() DAY()","title":"Fechas"},{"location":"sql/#numeros","text":"MIN() Elige el m\u00ednimo de un campo MAX() Elige un m\u00e1ximo de un campo COUNT() Cuenta la cantidad de registros de un campo SUM() Sumar\u00e1 los valores de un campo","title":"N\u00fameros"},{"location":"sql/#distinct","text":"SELECT DISTINCT campo FROM tabla /*Nos seleccionar\u00e1 los registros diferentes de un campo*/ SELECT COUNT (DISTINCT campo) FROM tabla /*Nos dir\u00e1 cu\u00e1ntos registros deferentes tenemos en un campo*/","title":"DISTINCT"},{"location":"sql/#group-by","text":"SELCT costo_renta, COUNT(*) as cantidad FROM pelicula GORUP BY costo_renta --------------------------- | costo_renta | cantidad | --------------------------- | 0.99 | 341 | | 2.99 | 323 | | 4.99 | 336 | --------------------------- Nos dir\u00e1 que tenemos 341 pel\u00edculas de costo 0.99, 323 pel\u00edculas de costo 2.99 y 336 pel\u00edculas de costo 4.99. OTRO EJEMPLO: SELECT tienda_id, COUNT(DISTINCT pelicula_id) AS peliculas, COUNT(inventario_id) AS copias FROM inventario GROUP BY tienda_id Nos dir\u00e1 cuantas pel\u00edculas hay por cada tienda y cuantas copias diferentes tambi\u00e9n de cada tienda. Tambi\u00e9n podemos agrupar por m\u00e1s de un campo. Nos agrupar\u00e1 todos los registros que tengan los dos campos en com\u00fan.","title":"GROUP BY"},{"location":"sql/#avg","text":"AVG(campo) /*Nos dir\u00e1 el promedio de un campo*/","title":"AVG"},{"location":"sql/#join","text":"Para cruzar tablas. (Devuelve registro que tienen registros en ambas tablas) SELECT tabla1.campo1, tabla1.campo2, tabla2.campo1 FROM tabla1 JOIN tabla2 ON tabla2.campo_igual = tabla1.campo_igual Podemos cambiar la notaci\u00f3n: SELECT A.campo1, A.campo2, B.campo1 FROM tabla1 A JOIN tabla2 B ON B.campo_igual = A.campo_igual Tambi\u00e9n podemos hacer JOIN con m\u00e1s de dos tablas. Escribiremos dos JOIN para ello.","title":"JOIN"},{"location":"sql/#left-join-y-right-join","text":"Si queremos todos los registros, aunque no est\u00e9n en ambas tablas. Left nos devolver\u00e1 todo lo que est\u00e1 en tabla1 aunque no est\u00e9 en tabla2. Right har\u00e1 lo contrario. EJEMPLO: Nos dir\u00e1 cu\u00e1ntas copias de cada pelicula (existente en la tienda1) hay en la tienda1: SELECT p.pelicula_id p.titulo COUNT(i.inventario_id) AS copias FROM pelicula p LEFT JOIN inventario i ON p.pelicula_id=i.pelicula_id WHERE i.tienda_id=1 GROUP BY p.pelicula_id, p.titulo --------------------------------------- | pelicula_id | cantidad | copias | --------------------------------------- | 1 | titulo1 | 8 | | 3 | titulo2 | 3 | | 6 | titulo3 | 4 | | ... | ... | ... | --------------------------------------- Sin utilizar WHERE : Nos dir\u00e1 cu\u00e1ntas copias de cada pelicula (existente en la tienda1) hay en la tienda1: SELECT p.pelicula_id p.titulo COUNT(i.inventario_id) AS copias FROM pelicula p LEFT JOIN inventario i ON p.pelicula_id=i.pelicula_id AND i.tienda_id=1 GROUP BY p.pelicula_id, p.titulo --------------------------------------- | pelicula_id | cantidad | copias | --------------------------------------- | 1 | titulo1 | 8 | | 2 | titulo2 | 0 | | 3 | titulo3 | 3 | | ... | ... | ... | --------------------------------------- CUIDADO AL UTILIZAR WHERE !!","title":"LEFT JOIN y RIGHT JOIN"},{"location":"sql/#condicionales","text":"SELECT campo1, campo2 CASE WHEN campo2<3 THEN 'peque\u00f1o' WHEN campo2>3 AND campo2<5 THEN 'MEDIANO' / (ELIF)Esta no se va a evaluar si la anterior ha sido correcta / ELSE 'grande' END AS tama\u00f1o FROM tabla","title":"CONDICIONALES"},{"location":"sql/#big-query","text":"Documentaci\u00f3n","title":"BIG QUERY"},{"location":"storage/","text":"GOOGLE STORAGE AUTORIZACI\u00d3N https://cloud.google.com/sdk/gcloud/reference/auth Cada cuenta tendr\u00e1 acceso a diversos proyectos. Habr\u00e1 cuentas con diferentes permisos. Para ver las cuentas con que est\u00e1n disponibles en local: gcloud auth list Para a\u00f1adir una cuenta a esta lista: gcloud auth login Para cambiar la cuenta con la que queremos que se conecte por defecto: gcloud auth application-default login Operar con archivos gsutil cp gsutil help cp Subir documentos En local: gsutil cp *.txt gs://my-bucket Descargar documentos En cloud: gsutil cp gs://my-bucket/*.txt Copiar documentos en la misma direcci\u00f3n de storage(?Proau bazpare) En la consola de cloud: gsutil cp -D gs://my-bucket/*.txt gsutil cp gs://my-bucket/*.txt Copiar repositorio En la consola de cloud: gsutil cp -r gs://my-bucket/*.txt gsutil cp gs://my-bucket/*.txt","title":"STORAGE"},{"location":"storage/#google-storage","text":"","title":"GOOGLE STORAGE"},{"location":"storage/#autorizacion","text":"https://cloud.google.com/sdk/gcloud/reference/auth Cada cuenta tendr\u00e1 acceso a diversos proyectos. Habr\u00e1 cuentas con diferentes permisos. Para ver las cuentas con que est\u00e1n disponibles en local: gcloud auth list Para a\u00f1adir una cuenta a esta lista: gcloud auth login Para cambiar la cuenta con la que queremos que se conecte por defecto: gcloud auth application-default login","title":"AUTORIZACI\u00d3N"},{"location":"storage/#operar-con-archivos","text":"gsutil cp gsutil help cp","title":"Operar con archivos"},{"location":"storage/#subir-documentos","text":"En local: gsutil cp *.txt gs://my-bucket","title":"Subir documentos"},{"location":"storage/#descargar-documentos","text":"En cloud: gsutil cp gs://my-bucket/*.txt","title":"Descargar documentos"},{"location":"storage/#copiar-documentos-en-la-misma-direccion-de-storageproau-bazpare","text":"En la consola de cloud: gsutil cp -D gs://my-bucket/*.txt gsutil cp gs://my-bucket/*.txt","title":"Copiar documentos en la misma direcci\u00f3n de storage(?Proau bazpare)"},{"location":"storage/#copiar-repositorio","text":"En la consola de cloud: gsutil cp -r gs://my-bucket/*.txt gsutil cp gs://my-bucket/*.txt","title":"Copiar repositorio"},{"location":"strapi/","text":"STRAPI npx create-strapi-app strapi-api Es una herramienta para la creaci\u00f3n de APIs que permitir\u00e1 al usuario-administradores cambiar las bases de datos desde una interfaz, sin tener que cambiar nada en el c\u00f3digo. Se contempla como opci\u00f3n de sustituci\u00f3n a Django, pero solo ser\u00e1 posible en proyectos simples sin muchos datos. Quickstart npx create-strapi-app@latest my-project --quickstart Una vez creado la app, nos ubicamos dentro de la carpeta que se nos ha creado con la creaci\u00f3n del proyecto y ejecutamos el siguiente comando para poner el servidor en marcha: npm run develop Panel admin en: http://localhost:1337/admin/ Collection type Acceder a Content-type Builder Desde ah\u00ed podremos crear nuevos \"modelos\" con sus atributos. Una vez creado el modelo, podemos a\u00f1adir los atributos deseados y crear tantos objetos de ese modelo como queramos. Acceder a los datos de la BD Los datos estar\u00e1n visibles en: http://localhost:1337/api/modelo_name Sin embargo strapi hace un filtro por defecto al obtener los datos. Si queremos acceder a todos tenemos que acceder por: http://localhost:1337/api/modelo_name?populate=* javascript: function getModelo(){ fetch('http://localhost:1337/api/modelo?populate=*') .then(response => response.json()) .then(data => data.data.forEach(readModelo)) } function readModelo(element){ var api_url = 'http://localhost:1337' var modelo = element.attributes var modelo_atrb1 = modelo.atrb1 var modelo_atrb2 = modelo.atrb2 var modelo_img_url = modelo.img_atrb.data.attributes.url modelo_img_url = api_url + img_url","title":"STRAPI"},{"location":"strapi/#strapi","text":"npx create-strapi-app strapi-api Es una herramienta para la creaci\u00f3n de APIs que permitir\u00e1 al usuario-administradores cambiar las bases de datos desde una interfaz, sin tener que cambiar nada en el c\u00f3digo. Se contempla como opci\u00f3n de sustituci\u00f3n a Django, pero solo ser\u00e1 posible en proyectos simples sin muchos datos.","title":"STRAPI"},{"location":"strapi/#quickstart","text":"npx create-strapi-app@latest my-project --quickstart Una vez creado la app, nos ubicamos dentro de la carpeta que se nos ha creado con la creaci\u00f3n del proyecto y ejecutamos el siguiente comando para poner el servidor en marcha: npm run develop Panel admin en: http://localhost:1337/admin/","title":"Quickstart"},{"location":"strapi/#collection-type","text":"Acceder a Content-type Builder Desde ah\u00ed podremos crear nuevos \"modelos\" con sus atributos. Una vez creado el modelo, podemos a\u00f1adir los atributos deseados y crear tantos objetos de ese modelo como queramos.","title":"Collection type"},{"location":"strapi/#acceder-a-los-datos-de-la-bd","text":"Los datos estar\u00e1n visibles en: http://localhost:1337/api/modelo_name Sin embargo strapi hace un filtro por defecto al obtener los datos. Si queremos acceder a todos tenemos que acceder por: http://localhost:1337/api/modelo_name?populate=* javascript: function getModelo(){ fetch('http://localhost:1337/api/modelo?populate=*') .then(response => response.json()) .then(data => data.data.forEach(readModelo)) } function readModelo(element){ var api_url = 'http://localhost:1337' var modelo = element.attributes var modelo_atrb1 = modelo.atrb1 var modelo_atrb2 = modelo.atrb2 var modelo_img_url = modelo.img_atrb.data.attributes.url modelo_img_url = api_url + img_url","title":"Acceder a los datos de la BD"}]}